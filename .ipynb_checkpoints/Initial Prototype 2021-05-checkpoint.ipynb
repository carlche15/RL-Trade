{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "952b8ddf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from data_handlers import StockHistDataHandler,ComputeSuite\n",
    "from trade_env_v0 import trade_env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "163cf48b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "# from tensorflow.keras import layers\n",
    "from tensorflow import keras \n",
    "from scipy.stats import norm, uniform\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "%config Completer.use_jedi = False\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.layers import Dense, GRU, TimeDistributed, Input, Masking\n",
    "\n",
    "from utilities.function_utilities import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5fa0c683",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_handler = StockHistDataHandler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "60572aea",
   "metadata": {},
   "outputs": [],
   "source": [
    "ts_range = [\"2019-01-15\",\"2021-07-5\"]\n",
    "raw_data = data_handler.get_data(sql= \"\"\"SELECT * from intraday_hist WHERE CAST([eff_date] as time) > CAST('9:30' as time) and CAST([eff_date] as time) < CAST('15:30' as time) order by eff_date asc\"\"\")\n",
    "# _ = ComputeSuite.pct_return(all_data.loc[:,:,:])\n",
    "raw_data = raw_data.loc[:,[\"adj_close_price\"],:]\n",
    "train_data = raw_data.values[:,:,~np.isnan(raw_data.values.sum(axis=0)[0])]\n",
    "train_data = train_data[::-1,:,:] # trading of snapchat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6384e545",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def state_sampler(state_hist_buffer, indices):\n",
    "    \"\"\"\n",
    "    given state history buffer and sample indices, return the data in the model input format\n",
    "    \"\"\"\n",
    "    sampled_time_series = np.concatenate([state_hist_buffer[i][0] for i in indices],axis=0)\n",
    "    sampled_current = np.concatenate([state_hist_buffer[i][1] for i in indices],axis=0)\n",
    "    return [sampled_time_series,sampled_current]\n",
    "\n",
    "def trade_model():\n",
    "    \n",
    "    ### timeseries sub_network\n",
    "    input_layer = keras.layers.Input(shape=(None,5,))\n",
    "    hidden_layer = keras.layers.GRU(20, activation=\"tanh\",return_sequences = True, name=\"gru1\")(input_layer)\n",
    "    hidden_layer = keras.layers.GRU(20, activation = \"tanh\", return_sequences =False, name = \"gru2\")(hidden_layer)\n",
    "    output_layer = keras.layers.Dense(2, activation=\"relu\", name =\"dense2\")(hidden_layer)\n",
    "    \n",
    "    \n",
    "    ### higher level vinalla neural network\n",
    "    input2 = keras.layers.Input(shape = (3))\n",
    "    concat = keras.layers.Concatenate()([output_layer, input2])\n",
    "    h2 = keras.layers.Dense(20,activation = \"relu\")(concat)\n",
    "    h2 = keras.layers.Dense(20,activation = \"relu\")(h2)\n",
    "    h2 = keras.layers.Dense(20,activation = \"relu\")(h2)\n",
    "    h2 = keras.layers.Dense(20,activation = \"relu\")(h2)\n",
    "    out2 = keras.layers.Dense(5, activation = \"linear\")(h2)\n",
    "     \n",
    "    model = keras.Model(inputs = [input_layer, input2], outputs = [out2])\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b9856412",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "main_model = trade_model()\n",
    "target_model = trade_model()\n",
    "loss_function = keras.losses.Huber() #Huber()\n",
    "# optimizer = keras.optimizers.Adam(learning_rate= 0.0000250,clipnorm=1.0)\n",
    "# optimizer = keras.optimizers.RMSprop(learning_rate= 0.00015)\n",
    "# main_model.summary()\n",
    "# tf.keras.utils.plot_model(\n",
    "#     main_model, to_file='model.png', show_shapes=True,\n",
    "#     show_layer_names=True, rankdir='TB', expand_nested=True, dpi=60\n",
    "# )\n",
    "\n",
    "start_ep = 0\n",
    "num_episode = 100000\n",
    "update_target = 500\n",
    "gamma = 1\n",
    "epi_sode_idx = 0\n",
    "exploring_state_boundary = 5000 # 1. explore boundary, 2. explore sub optimal\n",
    "exploring_state_sub_opt = 10000\n",
    "num_of_action = 5\n",
    "epsilon = 0.15\n",
    "prob_of_smaller_interval = 0.3\n",
    "time_elapsed = 0\n",
    "\n",
    "oversale_punish = 0.005\n",
    "reward_curve = []\n",
    "TWAP = []\n",
    "model_average = []\n",
    "\n",
    "\n",
    "\n",
    "# replay_buffer\n",
    "max_buffer_length = 20000\n",
    "action_history = []\n",
    "reward_history = []\n",
    "state_history = []\n",
    "state_next_history = []\n",
    "done_history = []\n",
    "update_after_action = 4\n",
    "batch_size = 32\n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "########## learning rates\n",
    "optimizer = keras.optimizers.RMSprop(learning_rate= 0.0001)\n",
    "step = tf.Variable(0, trainable=False)\n",
    "boundaries = [exploring_state_boundary, exploring_state_sub_opt]\n",
    "values = [0.006, 0.003, 0.0002]\n",
    "learning_rate_fn = keras.optimizers.schedules.PiecewiseConstantDecay(\n",
    "    boundaries, values)\n",
    "learning_rate = learning_rate_fn(step)\n",
    "optimizer = keras.optimizers.RMSprop(learning_rate= learning_rate)\n",
    "# optimizer = keras.optimizers.Adam(learning_rate= 0.00001,clipnorm=1.0)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2281be29",
   "metadata": {},
   "source": [
    "# load previous model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f3338567",
   "metadata": {},
   "outputs": [],
   "source": [
    "# previous_model = keras.models.load_model(\"13k_trace3\")\n",
    "# main_model.set_weights(previous_model.get_weights())\n",
    "# target_model.set_weights(previous_model.get_weights())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "5d6d1d65",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.61925  , 13.913995 ,  6.252237 , 14.974333 , -1.4513702]],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ep = 10022\n",
    "random_idx = 1000\n",
    "env = trade_env(data = train_data[:,:,random_idx+1:]/train_data[:,:,random_idx+20,None])\n",
    "env.inventory = 100\n",
    "env.time_remain = 5\n",
    "state1 = env.start()\n",
    "main_model.predict(state1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "431e8a0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training from ep 0\n",
      "100\n",
      "200\n",
      "ERROR:tensorflow:==================================\n",
      "Object was never used (type <class 'tensorflow.python.ops.tensor_array_ops.TensorArray'>):\n",
      "<tensorflow.python.ops.tensor_array_ops.TensorArray object at 0x0000013E4C2D9850>\n",
      "If you want to mark it as used call its \"mark_used()\" method.\n",
      "It was originally created here:\n",
      "  File \"C:\\DevelopmentTools\\envs\\quant\\lib\\site-packages\\tensorflow\\python\\keras\\backend.py\", line 4354, in <genexpr>\n",
      "    ta.write(time, out) for ta, out in zip(output_ta_t, flat_output))  File \"C:\\DevelopmentTools\\envs\\quant\\lib\\site-packages\\tensorflow\\python\\util\\tf_should_use.py\", line 247, in wrapped\n",
      "    return _add_should_use_warning(fn(*args, **kwargs),\n",
      "==================================\n",
      "300\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "print(f\"Training from ep {start_ep}\")\n",
    "for ep in range(start_ep, num_episode+start_ep):\n",
    "    \n",
    "    \n",
    "    ######## training cache  ############\n",
    "    ######################################\n",
    "\n",
    "    episode_reward = []\n",
    "    \n",
    "\n",
    "    ######## load episode data ############\n",
    "    ######################################\n",
    "    random_idx = np.random.randint(low=1, high = 160000)\n",
    "    data_slice =  train_data[:,:,random_idx+1:]/train_data[:,:,random_idx+20,None] # todo: +20 is hard coded to normalize the trade start pric\n",
    "    max_drawn = np.max(np.abs(np.diff(data_slice[:,0,:33],axis=1).ravel())) \n",
    "    if max_drawn>0.005:\n",
    "        # too volatile or cover two days...\n",
    "        continue\n",
    "        \n",
    "    env = trade_env(data =data_slice)\n",
    "    \n",
    "    ######## modify environment for small episode exploration ############\n",
    "    ######################################################################\n",
    "    if prob_of_smaller_interval>np.random.rand(1)[0] or ep<exploring_state_boundary:\n",
    "        \n",
    "        env.time_remain = np.random.choice([2,3,4])\n",
    "    else:\n",
    "        env.time_remain = 10\n",
    "        \n",
    "    current_state = env.start()\n",
    "    is_full_episode = True if env.time_remain ==10 else False\n",
    "    \n",
    "    \n",
    "    ######## exploration-stage and epsilon greedy  ############\n",
    "    ##########################################################\n",
    "    while True:\n",
    "        time_elapsed+=1\n",
    "        if time_elapsed%update_target ==0:\n",
    "                target_model.set_weights(main_model.get_weights())\n",
    "     \n",
    "        if ep< exploring_state_sub_opt or epsilon>np.random.rand(1)[0]:\n",
    "            action = np.random.choice(num_of_action)\n",
    "           \n",
    "        else:\n",
    "            action = tf.argmax(main_model(current_state,training = False),axis=1).numpy()[0]\n",
    "            \n",
    "           \n",
    "        \n",
    "        next_state, reward, end_episode = env.step(action)\n",
    "        \n",
    "        masks = tf.one_hot(action, num_of_action)\n",
    "        \n",
    "        action_history.append(action)\n",
    "        reward_history.append(reward)\n",
    "        state_history.append(current_state)\n",
    "        state_next_history.append(next_state)\n",
    "        done_history.append(end_episode)\n",
    "        episode_reward.append(reward)\n",
    "        \n",
    "        ######## train in the experience-replay buffer  ############\n",
    "        ##########################################################\n",
    "        \n",
    "        if  len(action_history)>= batch_size and time_elapsed%update_after_action==0:\n",
    "        \n",
    "            indices = np.random.choice(np.arange(len(action_history)),batch_size)\n",
    "            state_sample = state_sampler(state_history, indices)\n",
    "            state_next_sample = state_sampler(state_next_history, indices)\n",
    "            reward_sample = np.array(reward_history)[indices]\n",
    "            action_sample = np.array(action_history)[indices]\n",
    "            done_sample = np.array(done_history)[indices]\n",
    "\n",
    "            future_state_action_val = target_model.predict(state_next_sample)\n",
    "            future_state_action_val[np.isnan(future_state_action_val)] = 0 # this is for the terminal state (nans)\n",
    "            future_reward = tf.reduce_max(future_state_action_val,axis=1)\n",
    "            G = reward+gamma*future_reward\n",
    "            mask = tf.one_hot(action_sample,num_of_action)\n",
    "            \n",
    "#             print(f\"Act pre:{tf.reduce_sum(tf.multiply(main_model(state_sample),mask),axis=1)}\")\n",
    "            with tf.GradientTape() as tape:\n",
    "                current_state_action_val = main_model(state_sample)\n",
    "                current_reward = tf.reduce_sum(tf.multiply(current_state_action_val,mask),axis=1)\n",
    "                loss = loss_function(G, current_reward)  \n",
    "                grad = tape.gradient(loss, main_model.trainable_variables)\n",
    "                optimizer.apply_gradients(zip(grad, main_model.trainable_variables))\n",
    "#             print(f\"Act target:{G}\")\n",
    "#             print(f\"Act post:{tf.reduce_sum(tf.multiply(main_model(state_sample),mask),axis=1)}\")\n",
    "\n",
    "                \n",
    "                \n",
    "        ######## set_state, clear buffer, and end episode  ############\n",
    "        ##########################################################     \n",
    "        current_state = next_state\n",
    "        if len(action_history)>max_buffer_length:\n",
    "            del action_history[:1]\n",
    "            del state_history[:1]\n",
    "            del state_next_history[:1]\n",
    "            del done_history[:1]\n",
    "            del reward_history[:1]\n",
    "        \n",
    "        \n",
    "        if end_episode:\n",
    "            if ep%100 ==0:\n",
    "                print(ep)\n",
    "            \n",
    "            break\n",
    " \n",
    "\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bb25f81",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "sns.kdeplot(pnl)\n",
    "np.mean(pnl)*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24ef45a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(TWAP)\n",
    "plt.plot(model_average)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d323be65",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(model_average)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "293ccca7",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(TWAP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f0c27d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# main_model.save(\"13k_trace3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ade8c504",
   "metadata": {},
   "outputs": [],
   "source": [
    "a=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16d769c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(reward_curve)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1373bb3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "reward_curve = np.array(reward_curve)\n",
    "r = pd.DataFrame(reward_curve[reward_curve>99.5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2f49a3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "r.rolling(1500).mean().plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be0c2102",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:quant] *",
   "language": "python",
   "name": "conda-env-quant-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
