{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "163cf48b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "# from tensorflow.keras import layers\n",
    "from tensorflow import keras \n",
    "from scipy.stats import norm, uniform\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "%config Completer.use_jedi = False\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.layers import Dense, GRU, TimeDistributed, Input, Masking\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "6384e545",
   "metadata": {},
   "outputs": [],
   "source": [
    "def trade_model():\n",
    "    \n",
    "    ### timeseries sub_network\n",
    "    input_layer = keras.layers.Input(shape=(None,5,))\n",
    "    hidden_layer = keras.layers.GRU(20, activation=\"tanh\",return_sequences = True, name=\"gru1\")(input_layer)\n",
    "    hidden_layer = keras.layers.GRU(20, activation = \"tanh\", return_sequences =False, name = \"gru2\")(hidden_layer)\n",
    "    hidden_layer = keras.layers.Dense(5, activation =\"relu\",name=\"dense1\")(hidden_layer)\n",
    "    output_layer = keras.layers.Dense(2, activation=\"sigmoid\", name =\"dense2\")(hidden_layer)\n",
    "    \n",
    "    \n",
    "    ### higher level vinalla neural network\n",
    "    input2 = keras.layers.Input(shape = (3))\n",
    "    concat = keras.layers.Concatenate()([output_layer, input2])\n",
    "    h2 = keras.layers.Dense(10,activation = \"relu\")(concat)\n",
    "    out2 = keras.layers.Dense(11, activation = \"linear\")(h2)\n",
    "     \n",
    "    model = keras.Model(inputs = [input_layer, input2], outputs = [out2])\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a9bab4b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "16152250",
   "metadata": {},
   "outputs": [],
   "source": [
    "class trade_env():\n",
    "    def __init__(self, data,init_inventory = 100, time_range = 10, look_back = 20):\n",
    "        self.env_data = data\n",
    "        self.inventory = init_inventory\n",
    "        self.time_remain = time_range # todo: time unit\n",
    "        self.look_back = look_back # linked to model\n",
    "        self.current_time = look_back\n",
    "        self.current_price = None\n",
    "        \n",
    "        self.action_dir = {\n",
    "            0: 0,\n",
    "            1: 10,\n",
    "            2: 20,\n",
    "            3: 30,\n",
    "            4: 40,\n",
    "            5: 50,\n",
    "            6: 60,\n",
    "            7: 70,\n",
    "            8: 80,\n",
    "            9: 90,\n",
    "            10: 100,\n",
    "               \n",
    "            \n",
    "        }\n",
    "        \n",
    "        \n",
    "        \n",
    "    def reset(self):\n",
    "        \n",
    "        self.inventory = 100\n",
    "        self.time_remain = 10\n",
    "        \n",
    "\n",
    "    \n",
    "    \n",
    "    def start(self):\n",
    "        \"\"\"\n",
    "        return the beginin of state: 1. time_series state; 2. current state\n",
    "    \n",
    "        \"\"\"\n",
    "        # the original shape is num_of_security(feature) * num_of_price_attr(e.g. close, volume,etc) *time_steps\n",
    "        # reshape the original data into shape of  num_of_attr*time_steps* num_of_securities\n",
    "        time_series_input = self.env_data[:,:,self.current_time-self.look_back: self.current_time]\n",
    "        time_series_input = np.moveaxis(time_series_input, [1,0], [0,2])  \n",
    "        # current input consisits of current_price, inventory, and time left\n",
    "        self.current_price = time_series_input[:,-1,0].ravel()[0] # todo: the 1st is the security that we want to trade\n",
    "        current_input = np.array([self.current_price, self.inventory,self.time_remain])[None,:]\n",
    "        \n",
    "        \n",
    "        return [time_series_input,current_input]\n",
    "    \n",
    "    def step(self, action_choosen):\n",
    "        # reward based on previous observed price and action\n",
    "        \n",
    "        action = self.action_dir[int(action_choosen)]\n",
    "        reward = self.current_price*np.minimum(action, self.inventory)\n",
    "        end_of_epsoide = False\n",
    "        \n",
    "       \n",
    "        \n",
    "        if self.time_remain > 0: # if the previous action is executed with remaining time larger than 0\n",
    "            \n",
    "             # progress time and other variables # updates\n",
    "            self.current_time = self.current_time+1\n",
    "            self.inventory = np.maximum(self.inventory - action, 0)\n",
    "            self.time_remain = self.time_remain -1\n",
    "            \n",
    "            if self.inventory>0:# the trade hasn't finish and there are still time to do so\n",
    "                time_series_input = self.env_data[:,:,self.current_time-self.look_back: self.current_time]\n",
    "                time_series_input = np.moveaxis(time_series_input, [1,0], [0,2])  \n",
    "                self.current_price = time_series_input[:,-1,0].ravel()[0] # todo: the 1st is the security that we want to trade\n",
    "                current_input = np.array([self.current_price, self.inventory,self.time_remain])[None,:]\n",
    "                return [time_series_input,current_input], reward, end_of_epsoide\n",
    "           \n",
    "            else: # trade succssfully executed and get the reward as previous step\n",
    "                end_of_epsoide = True   \n",
    "                return None, reward, end_of_epsoide\n",
    "        \n",
    "        else: # failed to liquid the security within the target time, get a large negative punishment\n",
    "            \n",
    "            reward = -1000\n",
    "            end_of_epsoide = True\n",
    "            \n",
    "            return None, reward, end_of_epsoide\n",
    "                \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "b9856412",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"functional_13\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_13 (InputLayer)           [(None, None, 5)]    0                                            \n",
      "__________________________________________________________________________________________________\n",
      "gru1 (GRU)                      (None, None, 20)     1620        input_13[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "gru2 (GRU)                      (None, 20)           2520        gru1[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "dense1 (Dense)                  (None, 5)            105         gru2[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "dense2 (Dense)                  (None, 2)            12          dense1[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "input_14 (InputLayer)           [(None, 3)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_6 (Concatenate)     (None, 5)            0           dense2[0][0]                     \n",
      "                                                                 input_14[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_12 (Dense)                (None, 10)           60          concatenate_6[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_13 (Dense)                (None, 11)           121         dense_12[0][0]                   \n",
      "==================================================================================================\n",
      "Total params: 4,438\n",
      "Trainable params: 4,438\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "main_model = trade_model()\n",
    "target_model = trade_model()\n",
    "loss_function = keras.losses.Huber()\n",
    "optimizer = keras.optimizers.Adam(learning_rate= 0.0025)\n",
    "\n",
    "main_model.summary()\n",
    "tf.keras.utils.plot_model(\n",
    "    main_model, to_file='model.png', show_shapes=True,\n",
    "    show_layer_names=True, rankdir='TB', expand_nested=True, dpi=60\n",
    ")\n",
    "\n",
    "train_data = np.load(\"data/test_data.npy\")\n",
    "train_data = train_data/train_data[:,:,:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "d53cb8e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_episode = 300\n",
    "gamma = 0.98\n",
    "epi_sode_idx = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d2f31de",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "5d6d1d65",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-2.6489332, -4.9139256, -2.3581846,  2.8936474,  1.5217774,\n",
       "         2.4564261, -1.648513 ,  2.493589 , -1.0734471, -0.5959809,\n",
       "        -3.618737 ]], dtype=float32)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ep = 0\n",
    "env = trade_env(data = train_data[:,:,ep+1:])\n",
    "env.inventory = 10\n",
    "env.time_remain = 1\n",
    "state1 = env.start()\n",
    "main_model.predict(state1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24bdc892",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "f6bc3aa1",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "action taken: [3], reward:[35.82982869405424], future reward: [[20.597641]]\n",
      "action value before update: 29.622379302978516\n",
      "~~~~~~~~~ tf.Tensor([56.42747], shape=(1,), dtype=float32) tf.Tensor([29.62238], shape=(1,), dtype=float32)\n",
      "loss observed: 26.305091857910156\n",
      "action value after update: 30.255277633666992---3\n",
      "______________________________________\n",
      "action taken: [3], reward:[36.04121702453042], future reward: [[12.441946]]\n",
      "action value before update: 21.47549819946289\n",
      "~~~~~~~~~ tf.Tensor([48.483162], shape=(1,), dtype=float32) tf.Tensor([21.475498], shape=(1,), dtype=float32)\n",
      "loss observed: 26.50766372680664\n",
      "action value after update: 21.9268856048584---3\n",
      "______________________________________\n",
      "action taken: [3], reward:[36.290837783505786], future reward: [[3.941]]\n",
      "action value before update: 12.974372863769531\n",
      "~~~~~~~~~ tf.Tensor([40.231834], shape=(1,), dtype=float32) tf.Tensor([12.974373], shape=(1,), dtype=float32)\n",
      "loss observed: 26.757461547851562\n",
      "action value after update: 13.240575790405273---3\n",
      "______________________________________\n",
      "action taken: [3], reward:[12.072033184103532], future reward: [None]\n",
      "action value before update: 4.123544216156006\n",
      "~~~~~~~~~ 12.072033184103532 tf.Tensor([4.123544], shape=(1,), dtype=float32)\n",
      "loss observed: 7.448488712310791\n",
      "action value after update: 4.218482494354248---3\n",
      "______________________________________\n",
      "*********************************************\n",
      "action taken: [3], reward:[36.04121702453042], future reward: [[22.28529]]\n",
      "action value before update: 32.00035095214844\n",
      "~~~~~~~~~ tf.Tensor([58.326508], shape=(1,), dtype=float32) tf.Tensor([32.00035], shape=(1,), dtype=float32)\n",
      "loss observed: 25.826156616210938\n",
      "action value after update: 32.5717887878418---3\n",
      "______________________________________\n",
      "action taken: [10], reward:[84.67862149484684], future reward: [None]\n",
      "action value before update: -27.7193546295166\n",
      "~~~~~~~~~ 84.67862149484684 tf.Tensor([-27.719355], shape=(1,), dtype=float32)\n",
      "loss observed: 111.8979721069336\n",
      "action value after update: -27.651460647583008---10\n",
      "______________________________________\n",
      "*********************************************\n",
      "action taken: [3], reward:[36.290837783505786], future reward: [[22.98399]]\n",
      "action value before update: 32.986568450927734\n",
      "~~~~~~~~~ tf.Tensor([59.274826], shape=(1,), dtype=float32) tf.Tensor([32.98657], shape=(1,), dtype=float32)\n",
      "loss observed: 25.788257598876953\n",
      "action value after update: 33.46430969238281---3\n",
      "______________________________________\n",
      "action taken: [3], reward:[36.216099552310595], future reward: [[13.848473]]\n",
      "action value before update: 23.797346115112305\n",
      "~~~~~~~~~ tf.Tensor([50.06457], shape=(1,), dtype=float32) tf.Tensor([23.797346], shape=(1,), dtype=float32)\n",
      "loss observed: 25.76722526550293\n",
      "action value after update: 24.16362953186035---3\n",
      "______________________________________\n",
      "action taken: [3], reward:[36.5107359125834], future reward: [[4.4578137]]\n",
      "action value before update: 14.355491638183594\n",
      "~~~~~~~~~ tf.Tensor([40.968548], shape=(1,), dtype=float32) tf.Tensor([14.355492], shape=(1,), dtype=float32)\n",
      "loss observed: 26.113056182861328\n",
      "action value after update: 14.581183433532715---3\n",
      "______________________________________\n",
      "action taken: [3], reward:[12.499660841360088], future reward: [None]\n",
      "action value before update: 4.632818698883057\n",
      "~~~~~~~~~ 12.499660841360088 tf.Tensor([4.6328187], shape=(1,), dtype=float32)\n",
      "loss observed: 7.366841793060303\n",
      "action value after update: 4.714438438415527---3\n",
      "______________________________________\n",
      "*********************************************\n",
      "action taken: [3], reward:[36.216099552310595], future reward: [[24.379412]]\n",
      "action value before update: 34.95860290527344\n",
      "~~~~~~~~~ tf.Tensor([60.595512], shape=(1,), dtype=float32) tf.Tensor([34.958603], shape=(1,), dtype=float32)\n",
      "loss observed: 25.13690948486328\n",
      "action value after update: 35.48334884643555---3\n",
      "______________________________________\n",
      "action taken: [3], reward:[36.5107359125834], future reward: [[14.729666]]\n",
      "action value before update: 25.25600242614746\n",
      "~~~~~~~~~ tf.Tensor([51.240402], shape=(1,), dtype=float32) tf.Tensor([25.256002], shape=(1,), dtype=float32)\n",
      "loss observed: 25.484399795532227\n",
      "action value after update: 25.651628494262695---3\n",
      "______________________________________\n",
      "action taken: [3], reward:[37.49898252408026], future reward: [[4.794638]]\n",
      "action value before update: 15.27356243133545\n",
      "~~~~~~~~~ tf.Tensor([42.29362], shape=(1,), dtype=float32) tf.Tensor([15.273562], shape=(1,), dtype=float32)\n",
      "loss observed: 26.520057678222656\n",
      "action value after update: 15.516265869140625---3\n",
      "______________________________________\n",
      "action taken: [10], reward:[12.421633799111198], future reward: [None]\n",
      "action value before update: -3.5118885040283203\n",
      "~~~~~~~~~ 12.421633799111198 tf.Tensor([-3.5118885], shape=(1,), dtype=float32)\n",
      "loss observed: 15.43352222442627\n",
      "action value after update: -3.5234479904174805---10\n",
      "______________________________________\n",
      "*********************************************\n",
      "action taken: [3], reward:[36.5107359125834], future reward: [[25.870352]]\n",
      "action value before update: 37.063812255859375\n",
      "~~~~~~~~~ tf.Tensor([62.38109], shape=(1,), dtype=float32) tf.Tensor([37.063812], shape=(1,), dtype=float32)\n",
      "loss observed: 24.817276000976562\n",
      "action value after update: 37.59776306152344---3\n",
      "______________________________________\n",
      "action taken: [4], reward:[49.99864336544035], future reward: [[12.179056]]\n",
      "action value before update: 12.188531875610352\n",
      "~~~~~~~~~ tf.Tensor([62.177696], shape=(1,), dtype=float32) tf.Tensor([12.188532], shape=(1,), dtype=float32)\n",
      "loss observed: 49.489166259765625\n",
      "action value after update: 12.410381317138672---4\n",
      "______________________________________\n",
      "action taken: [3], reward:[37.264901397333595], future reward: [None]\n",
      "action value before update: 12.601801872253418\n",
      "~~~~~~~~~ 37.264901397333595 tf.Tensor([12.601802], shape=(1,), dtype=float32)\n",
      "loss observed: 24.163097381591797\n",
      "action value after update: 12.775684356689453---3\n",
      "______________________________________\n",
      "*********************************************\n",
      "action taken: [3], reward:[37.49898252408026], future reward: [[26.95077]]\n",
      "action value before update: 38.599212646484375\n",
      "~~~~~~~~~ tf.Tensor([64.44975], shape=(1,), dtype=float32) tf.Tensor([38.599213], shape=(1,), dtype=float32)\n",
      "loss observed: 25.350540161132812\n",
      "action value after update: 39.14533996582031---3\n",
      "______________________________________\n",
      "action taken: [8], reward:[86.95143659377838], future reward: [None]\n",
      "action value before update: -8.401662826538086\n",
      "~~~~~~~~~ 86.95143659377838 tf.Tensor([-8.401663], shape=(1,), dtype=float32)\n",
      "loss observed: 94.85310363769531\n",
      "action value after update: -8.242098808288574---8\n",
      "______________________________________\n",
      "*********************************************\n",
      "action taken: [3], reward:[37.264901397333595], future reward: [[27.679466]]\n",
      "action value before update: 39.6357536315918\n",
      "~~~~~~~~~ tf.Tensor([64.94437], shape=(1,), dtype=float32) tf.Tensor([39.635754], shape=(1,), dtype=float32)\n",
      "loss observed: 24.808612823486328\n",
      "action value after update: 40.17633819580078---3\n",
      "______________________________________\n",
      "action taken: [3], reward:[36.3119272843876], future reward: [[16.752298]]\n",
      "action value before update: 28.633041381835938\n",
      "~~~~~~~~~ tf.Tensor([53.064224], shape=(1,), dtype=float32) tf.Tensor([28.633041], shape=(1,), dtype=float32)\n",
      "loss observed: 23.931182861328125\n",
      "action value after update: 29.039066314697266---3\n",
      "______________________________________\n",
      "action taken: [3], reward:[36.770099774305336], future reward: [[5.528227]]\n",
      "action value before update: 17.341693878173828\n",
      "~~~~~~~~~ tf.Tensor([42.298325], shape=(1,), dtype=float32) tf.Tensor([17.341694], shape=(1,), dtype=float32)\n",
      "loss observed: 24.45663070678711\n",
      "action value after update: 17.589092254638672---3\n",
      "______________________________________\n",
      "action taken: [3], reward:[12.072978717281469], future reward: [None]\n",
      "action value before update: 5.730700969696045\n",
      "~~~~~~~~~ 12.072978717281469 tf.Tensor([5.730701], shape=(1,), dtype=float32)\n",
      "loss observed: 5.842278003692627\n",
      "action value after update: 5.817682266235352---3\n",
      "______________________________________\n",
      "*********************************************\n",
      "action taken: [3], reward:[36.3119272843876], future reward: [[29.232702]]\n",
      "action value before update: 41.832557678222656\n",
      "~~~~~~~~~ tf.Tensor([65.54463], shape=(1,), dtype=float32) tf.Tensor([41.832558], shape=(1,), dtype=float32)\n",
      "loss observed: 23.212074279785156\n",
      "action value after update: 42.412899017333984---3\n",
      "______________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "action taken: [3], reward:[36.770099774305336], future reward: [[17.71669]]\n",
      "action value before update: 30.247364044189453\n",
      "~~~~~~~~~ tf.Tensor([54.48679], shape=(1,), dtype=float32) tf.Tensor([30.247364], shape=(1,), dtype=float32)\n",
      "loss observed: 23.739425659179688\n",
      "action value after update: 30.681663513183594---3\n",
      "______________________________________\n",
      "action taken: [5], reward:[48.291914869125875], future reward: [None]\n",
      "action value before update: 9.995719909667969\n",
      "~~~~~~~~~ 48.291914869125875 tf.Tensor([9.99572], shape=(1,), dtype=float32)\n",
      "loss observed: 37.79619598388672\n",
      "action value after update: 10.11667537689209---5\n",
      "______________________________________\n",
      "*********************************************\n",
      "action taken: [3], reward:[36.770099774305336], future reward: [[30.459372]]\n",
      "action value before update: 43.574951171875\n",
      "~~~~~~~~~ tf.Tensor([67.22947], shape=(1,), dtype=float32) tf.Tensor([43.57495], shape=(1,), dtype=float32)\n",
      "loss observed: 23.154518127441406\n",
      "action value after update: 44.18052673339844---3\n",
      "______________________________________\n",
      "action taken: [3], reward:[36.2189361518444], future reward: [[18.478483]]\n",
      "action value before update: 31.516658782958984\n",
      "~~~~~~~~~ tf.Tensor([54.69742], shape=(1,), dtype=float32) tf.Tensor([31.516659], shape=(1,), dtype=float32)\n",
      "loss observed: 22.68075942993164\n",
      "action value after update: 31.968034744262695---3\n",
      "______________________________________\n",
      "action taken: [3], reward:[36.39653194874388], future reward: [[6.1677084]]\n",
      "action value before update: 19.131011962890625\n",
      "~~~~~~~~~ tf.Tensor([42.56424], shape=(1,), dtype=float32) tf.Tensor([19.131012], shape=(1,), dtype=float32)\n",
      "loss observed: 22.9332275390625\n",
      "action value after update: 19.40488052368164---3\n",
      "______________________________________\n",
      "action taken: [3], reward:[12.157542271499576], future reward: [None]\n",
      "action value before update: 6.393239974975586\n",
      "~~~~~~~~~ 12.157542271499576 tf.Tensor([6.39324], shape=(1,), dtype=float32)\n",
      "loss observed: 5.2643022537231445\n",
      "action value after update: 6.489593982696533---3\n",
      "______________________________________\n",
      "*********************************************\n",
      "action taken: [3], reward:[36.2189361518444], future reward: [[32.183853]]\n",
      "action value before update: 46.01560592651367\n",
      "~~~~~~~~~ tf.Tensor([68.40279], shape=(1,), dtype=float32) tf.Tensor([46.015606], shape=(1,), dtype=float32)\n",
      "loss observed: 21.88718032836914\n",
      "action value after update: 46.652870178222656---3\n",
      "______________________________________\n",
      "action taken: [3], reward:[36.39653194874388], future reward: [[19.547434]]\n",
      "action value before update: 33.299835205078125\n",
      "~~~~~~~~~ tf.Tensor([55.943962], shape=(1,), dtype=float32) tf.Tensor([33.299835], shape=(1,), dtype=float32)\n",
      "loss observed: 22.144126892089844\n",
      "action value after update: 33.774757385253906---3\n",
      "______________________________________\n",
      "action taken: [3], reward:[36.472626814498724], future reward: [[6.5638113]]\n",
      "action value before update: 20.236942291259766\n",
      "~~~~~~~~~ tf.Tensor([43.036438], shape=(1,), dtype=float32) tf.Tensor([20.236942], shape=(1,), dtype=float32)\n",
      "loss observed: 22.299495697021484\n",
      "action value after update: 20.52541732788086---3\n",
      "______________________________________\n",
      "action taken: [3], reward:[12.080501872566794], future reward: [None]\n",
      "action value before update: 6.80388879776001\n",
      "~~~~~~~~~ 12.080501872566794 tf.Tensor([6.803889], shape=(1,), dtype=float32)\n",
      "loss observed: 4.776612758636475\n",
      "action value after update: 6.906268119812012---3\n",
      "______________________________________\n",
      "*********************************************\n",
      "action taken: [3], reward:[36.39653194874388], future reward: [[33.99704]]\n",
      "action value before update: 48.582252502441406\n",
      "~~~~~~~~~ tf.Tensor([70.39357], shape=(1,), dtype=float32) tf.Tensor([48.582253], shape=(1,), dtype=float32)\n",
      "loss observed: 21.311317443847656\n",
      "action value after update: 49.250244140625---3\n",
      "______________________________________\n",
      "action taken: [3], reward:[36.472626814498724], future reward: [[20.671568]]\n",
      "action value before update: 35.172576904296875\n",
      "~~~~~~~~~ tf.Tensor([57.144196], shape=(1,), dtype=float32) tf.Tensor([35.172577], shape=(1,), dtype=float32)\n",
      "loss observed: 21.47161865234375\n",
      "action value after update: 35.67034149169922---3\n",
      "______________________________________\n",
      "action taken: [3], reward:[36.24150561770038], future reward: [[6.9823165]]\n",
      "action value before update: 21.39849281311035\n",
      "~~~~~~~~~ tf.Tensor([43.22382], shape=(1,), dtype=float32) tf.Tensor([21.398493], shape=(1,), dtype=float32)\n",
      "loss observed: 21.325326919555664\n",
      "action value after update: 21.700977325439453---3\n",
      "______________________________________\n",
      "action taken: [3], reward:[11.880295499673174], future reward: [None]\n",
      "action value before update: 7.236813068389893\n",
      "~~~~~~~~~ 11.880295499673174 tf.Tensor([7.236813], shape=(1,), dtype=float32)\n",
      "loss observed: 4.143482685089111\n",
      "action value after update: 7.344688415527344---3\n",
      "______________________________________\n",
      "*********************************************\n",
      "action taken: [3], reward:[36.472626814498724], future reward: [[35.89506]]\n",
      "action value before update: 51.26871109008789\n",
      "~~~~~~~~~ tf.Tensor([72.36769], shape=(1,), dtype=float32) tf.Tensor([51.26871], shape=(1,), dtype=float32)\n",
      "loss observed: 20.598979949951172\n",
      "action value after update: 51.96711730957031---3\n",
      "______________________________________\n",
      "action taken: [3], reward:[36.24150561770038], future reward: [[21.848055]]\n",
      "action value before update: 37.13151550292969\n",
      "~~~~~~~~~ tf.Tensor([58.08956], shape=(1,), dtype=float32) tf.Tensor([37.131516], shape=(1,), dtype=float32)\n",
      "loss observed: 20.458045959472656\n",
      "action value after update: 37.65188980102539---3\n",
      "______________________________________\n",
      "action taken: [3], reward:[35.64088649901952], future reward: [[7.4264665]]\n",
      "action value before update: 22.613155364990234\n",
      "~~~~~~~~~ tf.Tensor([43.067356], shape=(1,), dtype=float32) tf.Tensor([22.613155], shape=(1,), dtype=float32)\n",
      "loss observed: 19.954200744628906\n",
      "action value after update: 22.92940902709961---3\n",
      "______________________________________\n",
      "action taken: [3], reward:[11.898178409777636], future reward: [None]\n",
      "action value before update: 7.695727348327637\n",
      "~~~~~~~~~ 11.898178409777636 tf.Tensor([7.6957273], shape=(1,), dtype=float32)\n",
      "loss observed: 3.702450752258301\n",
      "action value after update: 7.808959007263184---3\n",
      "______________________________________\n",
      "*********************************************\n",
      "action taken: [3], reward:[36.24150561770038], future reward: [[37.876324]]\n",
      "action value before update: 54.073116302490234\n",
      "~~~~~~~~~ tf.Tensor([74.11783], shape=(1,), dtype=float32) tf.Tensor([54.073116], shape=(1,), dtype=float32)\n",
      "loss observed: 19.54471206665039\n",
      "action value after update: 54.802001953125---3\n",
      "______________________________________\n",
      "action taken: [3], reward:[35.64088649901952], future reward: [[23.081532]]\n",
      "action value before update: 39.17534255981445\n",
      "~~~~~~~~~ tf.Tensor([58.72242], shape=(1,), dtype=float32) tf.Tensor([39.175343], shape=(1,), dtype=float32)\n",
      "loss observed: 19.047077178955078\n",
      "action value after update: 39.71835708618164---3\n",
      "______________________________________\n",
      "action taken: [3], reward:[35.69453522933291], future reward: [[7.8932385]]\n",
      "action value before update: 23.88603973388672\n",
      "~~~~~~~~~ tf.Tensor([43.587772], shape=(1,), dtype=float32) tf.Tensor([23.88604], shape=(1,), dtype=float32)\n",
      "loss observed: 19.201732635498047\n",
      "action value after update: 24.216163635253906---3\n",
      "______________________________________\n",
      "action taken: [3], reward:[11.948908320280864], future reward: [None]\n",
      "action value before update: 8.177634239196777\n",
      "~~~~~~~~~ 11.948908320280864 tf.Tensor([8.177634], shape=(1,), dtype=float32)\n",
      "loss observed: 3.271273612976074\n",
      "action value after update: 8.296144485473633---3\n",
      "______________________________________\n",
      "*********************************************\n",
      "action taken: [3], reward:[35.64088649901952], future reward: [[39.94691]]\n",
      "action value before update: 56.99530792236328\n",
      "~~~~~~~~~ tf.Tensor([75.5878], shape=(1,), dtype=float32) tf.Tensor([56.995308], shape=(1,), dtype=float32)\n",
      "loss observed: 18.092491149902344\n",
      "action value after update: 57.75493240356445---3\n",
      "______________________________________\n",
      "action taken: [3], reward:[35.69453522933291], future reward: [[24.369629]]\n",
      "action value before update: 41.310630798339844\n",
      "~~~~~~~~~ tf.Tensor([60.064163], shape=(1,), dtype=float32) tf.Tensor([41.31063], shape=(1,), dtype=float32)\n",
      "loss observed: 18.25353240966797\n",
      "action value after update: 41.87662887573242---3\n",
      "______________________________________\n",
      "action taken: [3], reward:[35.84672496084259], future reward: [[8.375969]]\n",
      "action value before update: 25.214784622192383\n",
      "~~~~~~~~~ tf.Tensor([44.222694], shape=(1,), dtype=float32) tf.Tensor([25.214785], shape=(1,), dtype=float32)\n",
      "loss observed: 18.507909774780273\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "action value after update: 25.558927536010742---3\n",
      "______________________________________\n",
      "action taken: [3], reward:[11.701754169595764], future reward: [None]\n",
      "action value before update: 8.675660133361816\n",
      "~~~~~~~~~ 11.701754169595764 tf.Tensor([8.67566], shape=(1,), dtype=float32)\n",
      "loss observed: 2.526094436645508\n",
      "action value after update: 8.799285888671875---3\n",
      "______________________________________\n",
      "*********************************************\n",
      "action taken: [3], reward:[35.69453522933291], future reward: [[42.10514]]\n",
      "action value before update: 60.043304443359375\n",
      "~~~~~~~~~ tf.Tensor([77.799675], shape=(1,), dtype=float32) tf.Tensor([60.043304], shape=(1,), dtype=float32)\n",
      "loss observed: 17.256370544433594\n",
      "action value after update: 60.834228515625---3\n",
      "______________________________________\n",
      "action taken: [3], reward:[35.84672496084259], future reward: [[25.705988]]\n",
      "action value before update: 43.535648345947266\n",
      "~~~~~~~~~ tf.Tensor([61.55271], shape=(1,), dtype=float32) tf.Tensor([43.53565], shape=(1,), dtype=float32)\n",
      "loss observed: 17.51706314086914\n",
      "action value after update: 44.125003814697266---3\n",
      "______________________________________\n",
      "action taken: [3], reward:[35.10526250878729], future reward: [[8.880127]]\n",
      "action value before update: 26.592802047729492\n",
      "~~~~~~~~~ tf.Tensor([43.98539], shape=(1,), dtype=float32) tf.Tensor([26.592802], shape=(1,), dtype=float32)\n",
      "loss observed: 16.892587661743164\n",
      "action value after update: 26.950986862182617---3\n",
      "______________________________________\n",
      "action taken: [4], reward:[11.488392552487369], future reward: [None]\n",
      "action value before update: 4.935753345489502\n",
      "~~~~~~~~~ 11.488392552487369 tf.Tensor([4.9357533], shape=(1,), dtype=float32)\n",
      "loss observed: 6.052639484405518\n",
      "action value after update: 4.982194900512695---4\n",
      "______________________________________\n",
      "*********************************************\n",
      "action taken: [3], reward:[35.84672496084259], future reward: [[44.332447]]\n",
      "action value before update: 63.19906234741211\n",
      "~~~~~~~~~ tf.Tensor([80.17917], shape=(1,), dtype=float32) tf.Tensor([63.199062], shape=(1,), dtype=float32)\n",
      "loss observed: 16.480106353759766\n",
      "action value after update: 64.00670623779297---3\n",
      "______________________________________\n",
      "action taken: [3], reward:[35.10526250878729], future reward: [[27.081411]]\n",
      "action value before update: 45.81999588012695\n",
      "~~~~~~~~~ tf.Tensor([62.186676], shape=(1,), dtype=float32) tf.Tensor([45.819996], shape=(1,), dtype=float32)\n",
      "loss observed: 15.866680145263672\n",
      "action value after update: 46.42263412475586---3\n",
      "______________________________________\n",
      "action taken: [7], reward:[45.953570209949476], future reward: [None]\n",
      "action value before update: 12.650287628173828\n",
      "~~~~~~~~~ 45.953570209949476 tf.Tensor([12.650288], shape=(1,), dtype=float32)\n",
      "loss observed: 32.80328369140625\n",
      "action value after update: 12.863458633422852---7\n",
      "______________________________________\n",
      "*********************************************\n",
      "action taken: [3], reward:[35.10526250878729], future reward: [[46.036983]]\n",
      "action value before update: 65.61083221435547\n",
      "~~~~~~~~~ tf.Tensor([81.14224], shape=(1,), dtype=float32) tf.Tensor([65.61083], shape=(1,), dtype=float32)\n",
      "loss observed: 15.031410217285156\n",
      "action value after update: 66.45189666748047---3\n",
      "______________________________________\n",
      "action taken: [3], reward:[34.46517765746211], future reward: [[28.149881]]\n",
      "action value before update: 47.582794189453125\n",
      "~~~~~~~~~ tf.Tensor([62.61506], shape=(1,), dtype=float32) tf.Tensor([47.582794], shape=(1,), dtype=float32)\n",
      "loss observed: 14.532264709472656\n",
      "action value after update: 48.2083625793457---3\n",
      "______________________________________\n",
      "action taken: [3], reward:[35.08824291158442], future reward: [[9.799792]]\n",
      "action value before update: 29.10780906677246\n",
      "~~~~~~~~~ tf.Tensor([44.888035], shape=(1,), dtype=float32) tf.Tensor([29.10781], shape=(1,), dtype=float32)\n",
      "loss observed: 15.28022575378418\n",
      "action value after update: 29.48691749572754---3\n",
      "______________________________________\n",
      "action taken: [3], reward:[11.615258438883613], future reward: [None]\n",
      "action value before update: 10.140246391296387\n",
      "~~~~~~~~~ 11.615258438883613 tf.Tensor([10.140246], shape=(1,), dtype=float32)\n",
      "loss observed: 0.9750118255615234\n",
      "action value after update: 10.275016784667969---3\n",
      "______________________________________\n",
      "*********************************************\n",
      "action taken: [3], reward:[34.46517765746211], future reward: [[48.424793]]\n",
      "action value before update: 68.97538757324219\n",
      "~~~~~~~~~ tf.Tensor([82.88997], shape=(1,), dtype=float32) tf.Tensor([68.97539], shape=(1,), dtype=float32)\n",
      "loss observed: 13.414581298828125\n",
      "action value after update: 69.84922790527344---3\n",
      "______________________________________\n",
      "action taken: [3], reward:[35.08824291158442], future reward: [[29.625702]]\n",
      "action value before update: 50.043739318847656\n",
      "~~~~~~~~~ tf.Tensor([64.71394], shape=(1,), dtype=float32) tf.Tensor([50.04374], shape=(1,), dtype=float32)\n",
      "loss observed: 14.170204162597656\n",
      "action value after update: 50.69428634643555---3\n",
      "______________________________________\n",
      "action taken: [3], reward:[34.84577531665084], future reward: [[10.359482]]\n",
      "action value before update: 30.629661560058594\n",
      "~~~~~~~~~ tf.Tensor([45.205257], shape=(1,), dtype=float32) tf.Tensor([30.629662], shape=(1,), dtype=float32)\n",
      "loss observed: 14.07559585571289\n",
      "action value after update: 31.024269104003906---3\n",
      "______________________________________\n",
      "action taken: [3], reward:[11.743069858457796], future reward: [None]\n",
      "action value before update: 10.718257904052734\n",
      "~~~~~~~~~ 11.743069858457796 tf.Tensor([10.718258], shape=(1,), dtype=float32)\n",
      "loss observed: 0.5248117446899414\n",
      "action value after update: 10.859506607055664---3\n",
      "______________________________________\n",
      "*********************************************\n",
      "action taken: [3], reward:[35.08824291158442], future reward: [[50.899284]]\n",
      "action value before update: 72.48155212402344\n",
      "~~~~~~~~~ tf.Tensor([85.987526], shape=(1,), dtype=float32) tf.Tensor([72.48155], shape=(1,), dtype=float32)\n",
      "loss observed: 13.005973815917969\n",
      "action value after update: 73.388916015625---3\n",
      "______________________________________\n",
      "action taken: [3], reward:[34.84577531665084], future reward: [[31.16761]]\n",
      "action value before update: 52.59318542480469\n",
      "~~~~~~~~~ tf.Tensor([66.01338], shape=(1,), dtype=float32) tf.Tensor([52.593185], shape=(1,), dtype=float32)\n",
      "loss observed: 12.920196533203125\n",
      "action value after update: 53.268795013427734---3\n",
      "______________________________________\n",
      "action taken: [3], reward:[35.22920957537339], future reward: [[10.947955]]\n",
      "action value before update: 32.21904373168945\n",
      "~~~~~~~~~ tf.Tensor([46.177166], shape=(1,), dtype=float32) tf.Tensor([32.219044], shape=(1,), dtype=float32)\n",
      "loss observed: 13.458122253417969\n",
      "action value after update: 32.62926483154297---3\n",
      "______________________________________\n",
      "action taken: [3], reward:[11.939494098639665], future reward: [None]\n",
      "action value before update: 11.325435638427734\n",
      "~~~~~~~~~ 11.939494098639665 tf.Tensor([11.325436], shape=(1,), dtype=float32)\n",
      "loss observed: 0.18853391706943512\n",
      "action value after update: 11.47040843963623---3\n",
      "______________________________________\n",
      "*********************************************\n",
      "action taken: [3], reward:[34.84577531665084], future reward: [[53.467834]]\n",
      "action value before update: 76.10237121582031\n",
      "~~~~~~~~~ tf.Tensor([88.313614], shape=(1,), dtype=float32) tf.Tensor([76.10237], shape=(1,), dtype=float32)\n",
      "loss observed: 11.71124267578125\n",
      "action value after update: 77.03516387939453---3\n",
      "______________________________________\n",
      "action taken: [3], reward:[35.22920957537339], future reward: [[32.76364]]\n",
      "action value before update: 55.23262023925781\n",
      "~~~~~~~~~ tf.Tensor([67.99285], shape=(1,), dtype=float32) tf.Tensor([55.23262], shape=(1,), dtype=float32)\n",
      "loss observed: 12.260231018066406\n",
      "action value after update: 55.927955627441406---3\n",
      "______________________________________\n",
      "action taken: [3], reward:[35.818482295919], future reward: [[11.553942]]\n",
      "action value before update: 33.859867095947266\n",
      "~~~~~~~~~ tf.Tensor([47.37242], shape=(1,), dtype=float32) tf.Tensor([33.859867], shape=(1,), dtype=float32)\n",
      "loss observed: 13.012554168701172\n",
      "action value after update: 34.282310485839844---3\n",
      "______________________________________\n",
      "action taken: [3], reward:[12.073924250459406], future reward: [None]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "action value before update: 11.948477745056152\n",
      "~~~~~~~~~ 12.073924250459406 tf.Tensor([11.948478], shape=(1,), dtype=float32)\n",
      "loss observed: 0.00786838959902525\n",
      "action value after update: 12.094218254089355---3\n",
      "______________________________________\n",
      "*********************************************\n",
      "action taken: [3], reward:[35.22920957537339], future reward: [[56.10385]]\n",
      "action value before update: 79.82170867919922\n",
      "~~~~~~~~~ tf.Tensor([91.33306], shape=(1,), dtype=float32) tf.Tensor([79.82171], shape=(1,), dtype=float32)\n",
      "loss observed: 11.0113525390625\n",
      "action value after update: 80.77146911621094---3\n",
      "______________________________________\n",
      "action taken: [3], reward:[35.818482295919], future reward: [[34.393173]]\n",
      "action value before update: 57.934349060058594\n",
      "~~~~~~~~~ tf.Tensor([70.211655], shape=(1,), dtype=float32) tf.Tensor([57.93435], shape=(1,), dtype=float32)\n",
      "loss observed: 11.777305603027344\n",
      "action value after update: 58.64350891113281---3\n",
      "______________________________________\n",
      "action taken: [3], reward:[36.22177275137822], future reward: [[12.161679]]\n",
      "action value before update: 35.53069305419922\n",
      "~~~~~~~~~ tf.Tensor([48.38345], shape=(1,), dtype=float32) tf.Tensor([35.530693], shape=(1,), dtype=float32)\n",
      "loss observed: 12.35275650024414\n",
      "action value after update: 35.96155548095703---3\n",
      "______________________________________\n",
      "action taken: [3], reward:[11.98228975247586], future reward: [None]\n",
      "action value before update: 12.571045875549316\n",
      "~~~~~~~~~ 11.98228975247586 tf.Tensor([12.571046], shape=(1,), dtype=float32)\n",
      "loss observed: 0.17331714928150177\n",
      "action value after update: 12.713109016418457---3\n",
      "______________________________________\n",
      "*********************************************\n",
      "action taken: [3], reward:[35.818482295919], future reward: [[58.774734]]\n",
      "action value before update: 83.59818267822266\n",
      "~~~~~~~~~ tf.Tensor([94.593216], shape=(1,), dtype=float32) tf.Tensor([83.59818], shape=(1,), dtype=float32)\n",
      "loss observed: 10.495033264160156\n",
      "action value after update: 84.55191802978516---3\n",
      "______________________________________\n",
      "action taken: [3], reward:[36.22177275137822], future reward: [[36.027695]]\n",
      "action value before update: 60.66164016723633\n",
      "~~~~~~~~~ tf.Tensor([72.249466], shape=(1,), dtype=float32) tf.Tensor([60.66164], shape=(1,), dtype=float32)\n",
      "loss observed: 11.087825775146484\n",
      "action value after update: 61.375450134277344---3\n",
      "______________________________________\n",
      "action taken: [3], reward:[35.94686925742758], future reward: [[12.762135]]\n",
      "action value before update: 37.20012283325195\n",
      "~~~~~~~~~ tf.Tensor([48.709003], shape=(1,), dtype=float32) tf.Tensor([37.200123], shape=(1,), dtype=float32)\n",
      "loss observed: 11.008880615234375\n",
      "action value after update: 37.633426666259766---3\n",
      "______________________________________\n",
      "action taken: [3], reward:[11.832361078565585], future reward: [None]\n",
      "action value before update: 13.182865142822266\n",
      "~~~~~~~~~ 11.832361078565585 tf.Tensor([13.182865], shape=(1,), dtype=float32)\n",
      "loss observed: 0.8505039215087891\n",
      "action value after update: 13.320289611816406---3\n",
      "______________________________________\n",
      "*********************************************\n",
      "action taken: [3], reward:[36.22177275137822], future reward: [[61.44472]]\n",
      "action value before update: 87.38854217529297\n",
      "~~~~~~~~~ tf.Tensor([97.66649], shape=(1,), dtype=float32) tf.Tensor([87.38854], shape=(1,), dtype=float32)\n",
      "loss observed: 9.777946472167969\n",
      "action value after update: 88.3435287475586---3\n",
      "______________________________________\n",
      "action taken: [3], reward:[35.94686925742758], future reward: [[37.657207]]\n",
      "action value before update: 63.385780334472656\n",
      "~~~~~~~~~ tf.Tensor([73.60408], shape=(1,), dtype=float32) tf.Tensor([63.38578], shape=(1,), dtype=float32)\n",
      "loss observed: 9.718299865722656\n",
      "action value after update: 64.10208129882812---3\n",
      "______________________________________\n",
      "action taken: [3], reward:[35.497083235696756], future reward: [[13.362075]]\n",
      "action value before update: 38.862953186035156\n",
      "~~~~~~~~~ tf.Tensor([48.859158], shape=(1,), dtype=float32) tf.Tensor([38.862953], shape=(1,), dtype=float32)\n",
      "loss observed: 9.496204376220703\n",
      "action value after update: 39.29727554321289---3\n",
      "______________________________________\n",
      "action taken: [3], reward:[11.955485942388254], future reward: [None]\n",
      "action value before update: 13.793492317199707\n",
      "~~~~~~~~~ 11.955485942388254 tf.Tensor([13.793492], shape=(1,), dtype=float32)\n",
      "loss observed: 1.3380060195922852\n",
      "action value after update: 13.929207801818848---3\n",
      "______________________________________\n",
      "*********************************************\n",
      "action taken: [3], reward:[35.94686925742758], future reward: [[64.12046]]\n",
      "action value before update: 91.18766784667969\n",
      "~~~~~~~~~ tf.Tensor([100.06733], shape=(1,), dtype=float32) tf.Tensor([91.18767], shape=(1,), dtype=float32)\n",
      "loss observed: 8.379661560058594\n",
      "action value after update: 92.15248107910156---3\n",
      "______________________________________\n",
      "action taken: [3], reward:[35.497083235696756], future reward: [[39.30161]]\n",
      "action value before update: 66.12239074707031\n",
      "~~~~~~~~~ tf.Tensor([74.79869], shape=(1,), dtype=float32) tf.Tensor([66.12239], shape=(1,), dtype=float32)\n",
      "loss observed: 8.176300048828125\n",
      "action value after update: 66.84708404541016---3\n",
      "______________________________________\n",
      "action taken: [3], reward:[35.86645782716476], future reward: [[13.951355]]\n",
      "action value before update: 40.545223236083984\n",
      "~~~~~~~~~ tf.Tensor([49.817814], shape=(1,), dtype=float32) tf.Tensor([40.545223], shape=(1,), dtype=float32)\n",
      "loss observed: 8.772590637207031\n",
      "action value after update: 40.98444366455078---3\n",
      "______________________________________\n",
      "action taken: [3], reward:[11.906605988102728], future reward: [None]\n",
      "action value before update: 14.395158767700195\n",
      "~~~~~~~~~ 11.906605988102728 tf.Tensor([14.395159], shape=(1,), dtype=float32)\n",
      "loss observed: 1.9885530471801758\n",
      "action value after update: 14.53094482421875---3\n",
      "______________________________________\n",
      "*********************************************\n",
      "action taken: [3], reward:[35.497083235696756], future reward: [[66.83817]]\n",
      "action value before update: 95.03146362304688\n",
      "~~~~~~~~~ tf.Tensor([102.33525], shape=(1,), dtype=float32) tf.Tensor([95.03146], shape=(1,), dtype=float32)\n",
      "loss observed: 6.8037872314453125\n",
      "action value after update: 96.01185607910156---3\n",
      "______________________________________\n",
      "action taken: [3], reward:[35.86645782716476], future reward: [[40.957504]]\n",
      "action value before update: 68.90638732910156\n",
      "~~~~~~~~~ tf.Tensor([76.82396], shape=(1,), dtype=float32) tf.Tensor([68.90639], shape=(1,), dtype=float32)\n",
      "loss observed: 7.417572021484375\n",
      "action value after update: 69.64363098144531---3\n",
      "______________________________________\n",
      "action taken: [3], reward:[35.719817964308184], future reward: [[14.531883]]\n",
      "action value before update: 42.241859436035156\n",
      "~~~~~~~~~ tf.Tensor([50.2517], shape=(1,), dtype=float32) tf.Tensor([42.24186], shape=(1,), dtype=float32)\n",
      "loss observed: 7.5098419189453125\n",
      "action value after update: 42.68838119506836---3\n",
      "______________________________________\n",
      "action taken: [3], reward:[11.59552557256145], future reward: [None]\n",
      "action value before update: 14.989119529724121\n",
      "~~~~~~~~~ 11.59552557256145 tf.Tensor([14.98912], shape=(1,), dtype=float32)\n",
      "loss observed: 2.8935937881469727\n",
      "action value after update: 15.126115798950195---3\n",
      "______________________________________\n",
      "*********************************************\n",
      "action taken: [3], reward:[35.86645782716476], future reward: [[69.59482]]\n",
      "action value before update: 98.95478820800781\n",
      "~~~~~~~~~ tf.Tensor([105.46127], shape=(1,), dtype=float32) tf.Tensor([98.95479], shape=(1,), dtype=float32)\n",
      "loss observed: 6.0064849853515625\n",
      "action value after update: 99.95482635498047---3\n",
      "______________________________________\n",
      "action taken: [3], reward:[35.719817964308184], future reward: [[42.629177]]\n",
      "action value before update: 71.73301696777344\n",
      "~~~~~~~~~ tf.Tensor([78.349], shape=(1,), dtype=float32) tf.Tensor([71.73302], shape=(1,), dtype=float32)\n",
      "loss observed: 6.1159820556640625\n",
      "action value after update: 72.48542022705078---3\n",
      "______________________________________\n",
      "action taken: [3], reward:[34.78657671768435], future reward: [[15.136597]]\n",
      "action value before update: 43.956336975097656\n",
      "~~~~~~~~~ tf.Tensor([49.923172], shape=(1,), dtype=float32) tf.Tensor([43.956337], shape=(1,), dtype=float32)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss observed: 5.466835021972656\n",
      "action value after update: 44.41170120239258---3\n",
      "______________________________________\n",
      "action taken: [3], reward:[11.676348104205978], future reward: [None]\n",
      "action value before update: 15.608926773071289\n",
      "~~~~~~~~~ 11.676348104205978 tf.Tensor([15.608927], shape=(1,), dtype=float32)\n",
      "loss observed: 3.4325790405273438\n",
      "action value after update: 15.748136520385742---3\n",
      "______________________________________\n",
      "*********************************************\n",
      "action taken: [3], reward:[35.719817964308184], future reward: [[72.39529]]\n",
      "action value before update: 102.95177459716797\n",
      "~~~~~~~~~ tf.Tensor([108.115105], shape=(1,), dtype=float32) tf.Tensor([102.951775], shape=(1,), dtype=float32)\n",
      "loss observed: 4.663330078125\n",
      "action value after update: 103.97396850585938---3\n",
      "______________________________________\n",
      "action taken: [3], reward:[34.78657671768435], future reward: [[44.35216]]\n",
      "action value before update: 74.606201171875\n",
      "~~~~~~~~~ tf.Tensor([79.13873], shape=(1,), dtype=float32) tf.Tensor([74.6062], shape=(1,), dtype=float32)\n",
      "loss observed: 4.03253173828125\n",
      "action value after update: 75.3753890991211---3\n",
      "______________________________________\n",
      "action taken: [3], reward:[35.029044312617934], future reward: [[15.744044]]\n",
      "action value before update: 45.724605560302734\n",
      "~~~~~~~~~ tf.Tensor([50.77309], shape=(1,), dtype=float32) tf.Tensor([45.724606], shape=(1,), dtype=float32)\n",
      "loss observed: 4.548484802246094\n",
      "action value after update: 46.19021224975586---3\n",
      "______________________________________\n",
      "action taken: [3], reward:[11.583315861524612], future reward: [None]\n",
      "action value before update: 16.232072830200195\n",
      "~~~~~~~~~ 11.583315861524612 tf.Tensor([16.232073], shape=(1,), dtype=float32)\n",
      "loss observed: 4.148756980895996\n",
      "action value after update: 16.37399673461914---3\n",
      "______________________________________\n",
      "*********************************************\n",
      "action taken: [3], reward:[34.78657671768435], future reward: [[75.27662]]\n",
      "action value before update: 107.02597045898438\n",
      "~~~~~~~~~ tf.Tensor([110.063194], shape=(1,), dtype=float32) tf.Tensor([107.02597], shape=(1,), dtype=float32)\n",
      "loss observed: 2.5372238159179688\n",
      "action value after update: 108.07196044921875---3\n",
      "______________________________________\n",
      "action taken: [3], reward:[35.029044312617934], future reward: [[46.10532]]\n",
      "action value before update: 77.5634765625\n",
      "~~~~~~~~~ tf.Tensor([81.13437], shape=(1,), dtype=float32) tf.Tensor([77.56348], shape=(1,), dtype=float32)\n",
      "loss observed: 3.070892333984375\n",
      "action value after update: 78.35099792480469---3\n",
      "______________________________________\n",
      "action taken: [3], reward:[34.74994758457384], future reward: [[16.383303]]\n",
      "action value before update: 47.524497985839844\n",
      "~~~~~~~~~ tf.Tensor([51.133247], shape=(1,), dtype=float32) tf.Tensor([47.524498], shape=(1,), dtype=float32)\n",
      "loss observed: 3.1087493896484375\n",
      "action value after update: 48.001102447509766---3\n",
      "______________________________________\n",
      "action taken: [3], reward:[11.8765544770996], future reward: [None]\n",
      "action value before update: 16.888412475585938\n",
      "~~~~~~~~~ 11.8765544770996 tf.Tensor([16.888412], shape=(1,), dtype=float32)\n",
      "loss observed: 4.511857986450195\n",
      "action value after update: 17.03363609313965---3\n",
      "______________________________________\n",
      "*********************************************\n",
      "action taken: [3], reward:[35.029044312617934], future reward: [[78.21737]]\n",
      "action value before update: 111.21607208251953\n",
      "~~~~~~~~~ tf.Tensor([113.246414], shape=(1,), dtype=float32) tf.Tensor([111.21607], shape=(1,), dtype=float32)\n",
      "loss observed: 1.5303421020507812\n",
      "action value after update: 112.28741455078125---3\n",
      "______________________________________\n",
      "action taken: [4], reward:[46.33326344609845], future reward: [[37.82282]]\n",
      "action value before update: 25.4066219329834\n",
      "~~~~~~~~~ tf.Tensor([84.15608], shape=(1,), dtype=float32) tf.Tensor([25.406622], shape=(1,), dtype=float32)\n",
      "loss observed: 58.24945831298828\n",
      "action value after update: 25.89235496520996---4\n",
      "______________________________________\n",
      "action taken: [3], reward:[35.629663431298795], future reward: [None]\n",
      "action value before update: 38.938602447509766\n",
      "~~~~~~~~~ 35.629663431298795 tf.Tensor([38.938602], shape=(1,), dtype=float32)\n",
      "loss observed: 2.808940887451172\n",
      "action value after update: 39.22019577026367---3\n",
      "______________________________________\n",
      "*********************************************\n",
      "action taken: [3], reward:[34.74994758457384], future reward: [[80.27036]]\n",
      "action value before update: 114.1100845336914\n",
      "~~~~~~~~~ tf.Tensor([115.02031], shape=(1,), dtype=float32) tf.Tensor([114.110085], shape=(1,), dtype=float32)\n",
      "loss observed: 0.41425469517707825\n",
      "action value after update: 115.0549545288086---3\n",
      "______________________________________\n",
      "action taken: [3], reward:[35.629663431298795], future reward: [[49.08931]]\n",
      "action value before update: 82.5858383178711\n",
      "~~~~~~~~~ tf.Tensor([84.71897], shape=(1,), dtype=float32) tf.Tensor([82.58584], shape=(1,), dtype=float32)\n",
      "loss observed: 1.6331329345703125\n",
      "action value after update: 83.31232452392578---3\n",
      "______________________________________\n",
      "action taken: [3], reward:[35.08269304293132], future reward: [[17.404432]]\n",
      "action value before update: 50.531471252441406\n",
      "~~~~~~~~~ tf.Tensor([52.48712], shape=(1,), dtype=float32) tf.Tensor([50.53147], shape=(1,), dtype=float32)\n",
      "loss observed: 1.4556503295898438\n",
      "action value after update: 50.97559356689453---3\n",
      "______________________________________\n",
      "action taken: [1], reward:[11.677293637383917], future reward: [None]\n",
      "action value before update: -6.538058757781982\n",
      "~~~~~~~~~ 11.677293637383917 tf.Tensor([-6.5380588], shape=(1,), dtype=float32)\n",
      "loss observed: 17.71535301208496\n",
      "action value after update: -6.467341423034668---1\n",
      "______________________________________\n",
      "*********************************************\n",
      "action taken: [3], reward:[35.629663431298795], future reward: [[82.99672]]\n",
      "action value before update: 118.01589965820312\n",
      "~~~~~~~~~ tf.Tensor([118.62638], shape=(1,), dtype=float32) tf.Tensor([118.0159], shape=(1,), dtype=float32)\n",
      "loss observed: 0.18634368479251862\n",
      "action value after update: 118.98432159423828---3\n",
      "______________________________________\n",
      "action taken: [3], reward:[35.08269304293132], future reward: [[50.760883]]\n",
      "action value before update: 85.38594818115234\n",
      "~~~~~~~~~ tf.Tensor([85.843575], shape=(1,), dtype=float32) tf.Tensor([85.38595], shape=(1,), dtype=float32)\n",
      "loss observed: 0.10471093654632568\n",
      "action value after update: 86.07199096679688---3\n",
      "______________________________________\n",
      "action taken: [3], reward:[35.03188091215175], future reward: [[17.995464]]\n",
      "action value before update: 52.214046478271484\n",
      "~~~~~~~~~ tf.Tensor([53.027344], shape=(1,), dtype=float32) tf.Tensor([52.214046], shape=(1,), dtype=float32)\n",
      "loss observed: 0.3307262361049652\n",
      "action value after update: 52.6307373046875---3\n",
      "______________________________________\n",
      "action taken: [3], reward:[11.777849035350608], future reward: [None]\n",
      "action value before update: 18.513107299804688\n",
      "~~~~~~~~~ 11.777849035350608 tf.Tensor([18.513107], shape=(1,), dtype=float32)\n",
      "loss observed: 6.235258102416992\n",
      "action value after update: 18.638383865356445---3\n",
      "______________________________________\n",
      "*********************************************\n",
      "action taken: [3], reward:[35.08269304293132], future reward: [[85.59796]]\n",
      "action value before update: 121.70191955566406\n",
      "~~~~~~~~~ tf.Tensor([120.68065], shape=(1,), dtype=float32) tf.Tensor([121.70192], shape=(1,), dtype=float32)\n",
      "loss observed: 0.521270751953125\n",
      "action value after update: 122.19566345214844---3\n",
      "______________________________________\n",
      "action taken: [3], reward:[35.03188091215175], future reward: [[52.14444]]\n",
      "action value before update: 87.6987533569336\n",
      "~~~~~~~~~ tf.Tensor([87.176315], shape=(1,), dtype=float32) tf.Tensor([87.69875], shape=(1,), dtype=float32)\n",
      "loss observed: 0.136470764875412\n",
      "action value after update: 87.95330810546875---3\n",
      "______________________________________\n",
      "action taken: [3], reward:[35.333547106051824], future reward: [[18.400198]]\n",
      "action value before update: 53.3619270324707\n",
      "~~~~~~~~~ tf.Tensor([53.73374], shape=(1,), dtype=float32) tf.Tensor([53.361927], shape=(1,), dtype=float32)\n",
      "loss observed: 0.06912309676408768\n",
      "action value after update: 53.51873779296875---3\n",
      "______________________________________\n",
      "action taken: [3], reward:[11.968641186603028], future reward: [None]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "action value before update: 18.830299377441406\n",
      "~~~~~~~~~ 11.968641186603028 tf.Tensor([18.8303], shape=(1,), dtype=float32)\n",
      "loss observed: 6.361658096313477\n",
      "action value after update: 18.868471145629883---3\n",
      "______________________________________\n",
      "*********************************************\n",
      "action taken: [3], reward:[35.03188091215175], future reward: [[86.65503]]\n",
      "action value before update: 123.20066833496094\n",
      "~~~~~~~~~ tf.Tensor([121.686905], shape=(1,), dtype=float32) tf.Tensor([123.20067], shape=(1,), dtype=float32)\n",
      "loss observed: 1.013763427734375\n",
      "action value after update: 123.21672058105469---3\n",
      "______________________________________\n",
      "action taken: [3], reward:[35.333547106051824], future reward: [[52.581352]]\n",
      "action value before update: 88.43299102783203\n",
      "~~~~~~~~~ tf.Tensor([87.9149], shape=(1,), dtype=float32) tf.Tensor([88.43299], shape=(1,), dtype=float32)\n",
      "loss observed: 0.13420826196670532\n",
      "action value after update: 88.37686157226562---3\n",
      "______________________________________\n",
      "action taken: [3], reward:[35.90592355980908], future reward: [[18.462715]]\n",
      "action value before update: 53.617332458496094\n",
      "~~~~~~~~~ tf.Tensor([54.368637], shape=(1,), dtype=float32) tf.Tensor([53.617332], shape=(1,), dtype=float32)\n",
      "loss observed: 0.28222933411598206\n",
      "action value after update: 53.620269775390625---3\n",
      "______________________________________\n",
      "action taken: [3], reward:[11.842720833384721], future reward: [None]\n",
      "action value before update: 18.83816146850586\n",
      "~~~~~~~~~ 11.842720833384721 tf.Tensor([18.838161], shape=(1,), dtype=float32)\n",
      "loss observed: 6.495440483093262\n",
      "action value after update: 18.82612419128418---3\n",
      "______________________________________\n",
      "*********************************************\n",
      "action taken: [3], reward:[35.333547106051824], future reward: [[86.605515]]\n",
      "action value before update: 123.13177490234375\n",
      "~~~~~~~~~ tf.Tensor([121.93906], shape=(1,), dtype=float32) tf.Tensor([123.131775], shape=(1,), dtype=float32)\n",
      "loss observed: 0.692718505859375\n",
      "action value after update: 122.86893463134766---3\n",
      "______________________________________\n",
      "action taken: [3], reward:[35.90592355980908], future reward: [[52.40696]]\n",
      "action value before update: 88.18180847167969\n",
      "~~~~~~~~~ tf.Tensor([88.31288], shape=(1,), dtype=float32) tf.Tensor([88.18181], shape=(1,), dtype=float32)\n",
      "loss observed: 0.008590064942836761\n",
      "action value after update: 88.02555847167969---3\n",
      "______________________________________\n",
      "action taken: [3], reward:[35.52816250015417], future reward: [[18.362154]]\n",
      "action value before update: 53.37872314453125\n",
      "~~~~~~~~~ tf.Tensor([53.89032], shape=(1,), dtype=float32) tf.Tensor([53.378723], shape=(1,), dtype=float32)\n",
      "loss observed: 0.13086557388305664\n",
      "action value after update: 53.31503677368164---3\n",
      "______________________________________\n",
      "action taken: [3], reward:[11.80415952378016], future reward: [None]\n",
      "action value before update: 18.711454391479492\n",
      "~~~~~~~~~ 11.80415952378016 tf.Tensor([18.711454], shape=(1,), dtype=float32)\n",
      "loss observed: 6.407295227050781\n",
      "action value after update: 18.6779727935791---3\n",
      "______________________________________\n",
      "*********************************************\n",
      "action taken: [3], reward:[35.90592355980908], future reward: [[86.040306]]\n",
      "action value before update: 122.3614273071289\n",
      "~~~~~~~~~ tf.Tensor([121.94623], shape=(1,), dtype=float32) tf.Tensor([122.36143], shape=(1,), dtype=float32)\n",
      "loss observed: 0.08619522303342819\n",
      "action value after update: 122.11454772949219---3\n",
      "______________________________________\n",
      "action taken: [3], reward:[35.52816250015417], future reward: [[52.060936]]\n",
      "action value before update: 87.61649322509766\n",
      "~~~~~~~~~ tf.Tensor([87.589096], shape=(1,), dtype=float32) tf.Tensor([87.61649], shape=(1,), dtype=float32)\n",
      "loss observed: 0.00037530207191593945\n",
      "action value after update: 87.45088958740234---3\n",
      "______________________________________\n",
      "action taken: [3], reward:[35.41247857134048], future reward: [[18.223907]]\n",
      "action value before update: 53.01971435546875\n",
      "~~~~~~~~~ tf.Tensor([53.636387], shape=(1,), dtype=float32) tf.Tensor([53.019714], shape=(1,), dtype=float32)\n",
      "loss observed: 0.19014249742031097\n",
      "action value after update: 52.955780029296875---3\n",
      "______________________________________\n",
      "action taken: [3], reward:[11.894396277065889], future reward: [None]\n",
      "action value before update: 18.570144653320312\n",
      "~~~~~~~~~ 11.894396277065889 tf.Tensor([18.570145], shape=(1,), dtype=float32)\n",
      "loss observed: 6.175748825073242\n",
      "action value after update: 18.536527633666992---3\n",
      "______________________________________\n",
      "*********************************************\n",
      "action taken: [3], reward:[35.52816250015417], future reward: [[85.481606]]\n",
      "action value before update: 121.57563781738281\n",
      "~~~~~~~~~ tf.Tensor([121.009766], shape=(1,), dtype=float32) tf.Tensor([121.57564], shape=(1,), dtype=float32)\n",
      "loss observed: 0.16010567545890808\n",
      "action value after update: 121.29249572753906---3\n",
      "______________________________________\n",
      "action taken: [3], reward:[35.41247857134048], future reward: [[51.70032]]\n",
      "action value before update: 87.02034759521484\n",
      "~~~~~~~~~ tf.Tensor([87.1128], shape=(1,), dtype=float32) tf.Tensor([87.02035], shape=(1,), dtype=float32)\n",
      "loss observed: 0.004273778758943081\n",
      "action value after update: 86.84619140625---3\n",
      "______________________________________\n",
      "action taken: [3], reward:[35.68318883119767], future reward: [[18.075935]]\n",
      "action value before update: 52.64661407470703\n",
      "~~~~~~~~~ tf.Tensor([53.759125], shape=(1,), dtype=float32) tf.Tensor([52.646614], shape=(1,), dtype=float32)\n",
      "loss observed: 0.6125106811523438\n",
      "action value after update: 52.596500396728516---3\n",
      "______________________________________\n",
      "action taken: [3], reward:[11.94516729770729], future reward: [None]\n",
      "action value before update: 18.425018310546875\n",
      "~~~~~~~~~ 11.94516729770729 tf.Tensor([18.425018], shape=(1,), dtype=float32)\n",
      "loss observed: 5.979850769042969\n",
      "action value after update: 18.39680290222168---3\n",
      "______________________________________\n",
      "*********************************************\n",
      "action taken: [3], reward:[35.41247857134048], future reward: [[84.93677]]\n",
      "action value before update: 120.80325317382812\n",
      "~~~~~~~~~ tf.Tensor([120.34924], shape=(1,), dtype=float32) tf.Tensor([120.80325], shape=(1,), dtype=float32)\n",
      "loss observed: 0.10306254774332047\n",
      "action value after update: 120.57072448730469---3\n",
      "______________________________________\n",
      "action taken: [3], reward:[35.68318883119767], future reward: [[51.38071]]\n",
      "action value before update: 86.5010986328125\n",
      "~~~~~~~~~ tf.Tensor([87.0639], shape=(1,), dtype=float32) tf.Tensor([86.5011], shape=(1,), dtype=float32)\n",
      "loss observed: 0.1583748310804367\n",
      "action value after update: 86.41900634765625---3\n",
      "______________________________________\n",
      "action taken: [3], reward:[35.835501893121865], future reward: [[17.964603]]\n",
      "action value before update: 52.37717056274414\n",
      "~~~~~~~~~ tf.Tensor([53.800106], shape=(1,), dtype=float32) tf.Tensor([52.37717], shape=(1,), dtype=float32)\n",
      "loss observed: 0.9229354858398438\n",
      "action value after update: 52.37797546386719---3\n",
      "______________________________________\n",
      "action taken: [3], reward:[11.921652298673378], future reward: [None]\n",
      "action value before update: 18.330467224121094\n",
      "~~~~~~~~~ 11.921652298673378 tf.Tensor([18.330467], shape=(1,), dtype=float32)\n",
      "loss observed: 5.908815383911133\n",
      "action value after update: 18.319488525390625---3\n",
      "______________________________________\n",
      "*********************************************\n",
      "action taken: [3], reward:[35.68318883119767], future reward: [[84.67034]]\n",
      "action value before update: 120.43427276611328\n",
      "~~~~~~~~~ tf.Tensor([120.35353], shape=(1,), dtype=float32) tf.Tensor([120.43427], shape=(1,), dtype=float32)\n",
      "loss observed: 0.0032596257515251637\n",
      "action value after update: 120.3846206665039---3\n",
      "______________________________________\n",
      "action taken: [3], reward:[35.835501893121865], future reward: [[51.286434]]\n",
      "action value before update: 86.36090087890625\n",
      "~~~~~~~~~ tf.Tensor([87.12193], shape=(1,), dtype=float32) tf.Tensor([86.3609], shape=(1,), dtype=float32)\n",
      "loss observed: 0.2895849347114563\n",
      "action value after update: 86.42256164550781---3\n",
      "______________________________________\n",
      "action taken: [3], reward:[35.76495689602013], future reward: [[17.947239]]\n",
      "action value before update: 52.368804931640625\n",
      "~~~~~~~~~ tf.Tensor([53.712196], shape=(1,), dtype=float32) tf.Tensor([52.368805], shape=(1,), dtype=float32)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss observed: 0.8433914184570312\n",
      "action value after update: 52.448970794677734---3\n",
      "______________________________________\n",
      "action taken: [3], reward:[11.86997685499221], future reward: [None]\n",
      "action value before update: 18.34149742126465\n",
      "~~~~~~~~~ 11.86997685499221 tf.Tensor([18.341497], shape=(1,), dtype=float32)\n",
      "loss observed: 5.97152042388916\n",
      "action value after update: 18.3564510345459---3\n",
      "______________________________________\n",
      "*********************************************\n",
      "action taken: [3], reward:[35.835501893121865], future reward: [[84.91207]]\n",
      "action value before update: 120.78910064697266\n",
      "~~~~~~~~~ tf.Tensor([120.747574], shape=(1,), dtype=float32) tf.Tensor([120.7891], shape=(1,), dtype=float32)\n",
      "loss observed: 0.000862237298861146\n",
      "action value after update: 120.89604949951172---3\n",
      "______________________________________\n",
      "action taken: [3], reward:[35.76495689602013], future reward: [[51.492054]]\n",
      "action value before update: 86.72025299072266\n",
      "~~~~~~~~~ tf.Tensor([87.25701], shape=(1,), dtype=float32) tf.Tensor([86.72025], shape=(1,), dtype=float32)\n",
      "loss observed: 0.14405480027198792\n",
      "action value after update: 86.8562240600586---3\n",
      "______________________________________\n",
      "action taken: [3], reward:[35.60993056497663], future reward: [[18.028982]]\n",
      "action value before update: 52.623897552490234\n",
      "~~~~~~~~~ tf.Tensor([53.638916], shape=(1,), dtype=float32) tf.Tensor([52.623898], shape=(1,), dtype=float32)\n",
      "loss observed: 0.5150184631347656\n",
      "action value after update: 52.7454948425293---3\n",
      "______________________________________\n",
      "action taken: [3], reward:[11.91507467656599], future reward: [None]\n",
      "action value before update: 18.439754486083984\n",
      "~~~~~~~~~ 11.91507467656599 tf.Tensor([18.439754], shape=(1,), dtype=float32)\n",
      "loss observed: 6.024680137634277\n",
      "action value after update: 18.468021392822266---3\n",
      "______________________________________\n",
      "*********************************************\n",
      "action taken: [3], reward:[35.76495689602013], future reward: [[85.46213]]\n",
      "action value before update: 121.5794906616211\n",
      "~~~~~~~~~ tf.Tensor([121.22708], shape=(1,), dtype=float32) tf.Tensor([121.57949], shape=(1,), dtype=float32)\n",
      "loss observed: 0.06209617853164673\n",
      "action value after update: 121.68567657470703---3\n",
      "______________________________________\n",
      "action taken: [3], reward:[35.60993056497663], future reward: [[51.82428]]\n",
      "action value before update: 87.28111267089844\n",
      "~~~~~~~~~ tf.Tensor([87.43421], shape=(1,), dtype=float32) tf.Tensor([87.28111], shape=(1,), dtype=float32)\n",
      "loss observed: 0.011719660833477974\n",
      "action value after update: 87.36845397949219---3\n",
      "______________________________________\n",
      "action taken: [3], reward:[35.745224029697965], future reward: [[18.122618]]\n",
      "action value before update: 52.93330001831055\n",
      "~~~~~~~~~ tf.Tensor([53.867844], shape=(1,), dtype=float32) tf.Tensor([52.9333], shape=(1,), dtype=float32)\n",
      "loss observed: 0.4366858899593353\n",
      "action value after update: 53.025672912597656---3\n",
      "______________________________________\n",
      "action taken: [3], reward:[11.857767143955371], future reward: [None]\n",
      "action value before update: 18.52471923828125\n",
      "~~~~~~~~~ 11.857767143955371 tf.Tensor([18.52472], shape=(1,), dtype=float32)\n",
      "loss observed: 6.166952133178711\n",
      "action value after update: 18.543319702148438---3\n",
      "______________________________________\n",
      "*********************************************\n",
      "action taken: [2], reward:[23.73995370998442], future reward: [[96.888885]]\n",
      "action value before update: -32.98253631591797\n",
      "~~~~~~~~~ tf.Tensor([120.62884], shape=(1,), dtype=float32) tf.Tensor([-32.982536], shape=(1,), dtype=float32)\n",
      "loss observed: 153.1113739013672\n",
      "action value after update: -32.2489128112793---2\n",
      "______________________________________\n",
      "action taken: [3], reward:[35.745224029697965], future reward: [[63.088577]]\n",
      "action value before update: 98.95494079589844\n",
      "~~~~~~~~~ tf.Tensor([98.8338], shape=(1,), dtype=float32) tf.Tensor([98.95494], shape=(1,), dtype=float32)\n",
      "loss observed: 0.007337392307817936\n",
      "action value after update: 99.0150375366211---3\n",
      "______________________________________\n",
      "action taken: [3], reward:[35.57330143186611], future reward: [[29.211733]]\n",
      "action value before update: 64.41357421875\n",
      "~~~~~~~~~ tf.Tensor([64.785034], shape=(1,), dtype=float32) tf.Tensor([64.413574], shape=(1,), dtype=float32)\n",
      "loss observed: 0.06899125128984451\n",
      "action value after update: 64.47396087646484---3\n",
      "______________________________________\n",
      "action taken: [3], reward:[23.448565050627135], future reward: [None]\n",
      "action value before update: 29.834157943725586\n",
      "~~~~~~~~~ 23.448565050627135 tf.Tensor([29.834158], shape=(1,), dtype=float32)\n",
      "loss observed: 5.885593414306641\n",
      "action value after update: 29.839336395263672---3\n",
      "______________________________________\n",
      "*********************************************\n",
      "action taken: [3], reward:[35.745224029697965], future reward: [[86.11492]]\n",
      "action value before update: 122.52257537841797\n",
      "~~~~~~~~~ tf.Tensor([121.860146], shape=(1,), dtype=float32) tf.Tensor([122.522575], shape=(1,), dtype=float32)\n",
      "loss observed: 0.2194066196680069\n",
      "action value after update: 122.39442443847656---3\n",
      "______________________________________\n",
      "action taken: [3], reward:[35.57330143186611], future reward: [[52.095184]]\n",
      "action value before update: 87.77849578857422\n",
      "~~~~~~~~~ tf.Tensor([87.66849], shape=(1,), dtype=float32) tf.Tensor([87.778496], shape=(1,), dtype=float32)\n",
      "loss observed: 0.0060509066097438335\n",
      "action value after update: 87.67938232421875---3\n",
      "______________________________________\n",
      "action taken: [3], reward:[35.1728475759407], future reward: [[18.16372]]\n",
      "action value before update: 53.09580612182617\n",
      "~~~~~~~~~ tf.Tensor([53.336567], shape=(1,), dtype=float32) tf.Tensor([53.095806], shape=(1,), dtype=float32)\n",
      "loss observed: 0.02898288145661354\n",
      "action value after update: 53.05140686035156---3\n",
      "______________________________________\n",
      "action taken: [3], reward:[11.99215618563694], future reward: [None]\n",
      "action value before update: 18.515703201293945\n",
      "~~~~~~~~~ 11.99215618563694 tf.Tensor([18.515703], shape=(1,), dtype=float32)\n",
      "loss observed: 6.023547172546387\n",
      "action value after update: 18.48824691772461---3\n",
      "______________________________________\n",
      "*********************************************\n",
      "action taken: [3], reward:[35.57330143186611], future reward: [[85.759254]]\n",
      "action value before update: 122.03475189208984\n",
      "~~~~~~~~~ tf.Tensor([121.33256], shape=(1,), dtype=float32) tf.Tensor([122.03475], shape=(1,), dtype=float32)\n",
      "loss observed: 0.24653835594654083\n",
      "action value after update: 121.74501037597656---3\n",
      "______________________________________\n",
      "action taken: [3], reward:[35.1728475759407], future reward: [[51.817772]]\n",
      "action value before update: 87.29915618896484\n",
      "~~~~~~~~~ tf.Tensor([86.990616], shape=(1,), dtype=float32) tf.Tensor([87.29916], shape=(1,), dtype=float32)\n",
      "loss observed: 0.047598570585250854\n",
      "action value after update: 87.06932830810547---3\n",
      "______________________________________\n",
      "action taken: [3], reward:[35.97646855691082], future reward: [[18.010231]]\n",
      "action value before update: 52.732635498046875\n",
      "~~~~~~~~~ tf.Tensor([53.9867], shape=(1,), dtype=float32) tf.Tensor([52.732635], shape=(1,), dtype=float32)\n",
      "loss observed: 0.7540626525878906\n",
      "action value after update: 52.65414810180664---3\n",
      "______________________________________\n",
      "action taken: [3], reward:[11.927284387602828], future reward: [None]\n",
      "action value before update: 18.347793579101562\n",
      "~~~~~~~~~ 11.927284387602828 tf.Tensor([18.347794], shape=(1,), dtype=float32)\n",
      "loss observed: 5.920509338378906\n",
      "action value after update: 18.31031036376953---3\n",
      "______________________________________\n",
      "*********************************************\n",
      "action taken: [3], reward:[35.1728475759407], future reward: [[85.077255]]\n",
      "action value before update: 121.04883575439453\n",
      "~~~~~~~~~ tf.Tensor([120.25011], shape=(1,), dtype=float32) tf.Tensor([121.048836], shape=(1,), dtype=float32)\n",
      "loss observed: 0.3189839720726013\n",
      "action value after update: 120.6729965209961---3\n",
      "______________________________________\n",
      "action taken: [3], reward:[35.97646855691082], future reward: [[51.344486]]\n",
      "action value before update: 86.5416488647461\n",
      "~~~~~~~~~ tf.Tensor([87.32095], shape=(1,), dtype=float32) tf.Tensor([86.54165], shape=(1,), dtype=float32)\n",
      "loss observed: 0.30365777015686035\n",
      "action value after update: 86.39813995361328---3\n",
      "______________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "action taken: [3], reward:[35.78185316280848], future reward: [[17.855803]]\n",
      "action value before update: 52.303104400634766\n",
      "~~~~~~~~~ tf.Tensor([53.637657], shape=(1,), dtype=float32) tf.Tensor([52.303104], shape=(1,), dtype=float32)\n",
      "loss observed: 0.8345527648925781\n",
      "action value after update: 52.27193069458008---3\n",
      "______________________________________\n",
      "action taken: [3], reward:[12.088025027852119], future reward: [None]\n",
      "action value before update: 18.208232879638672\n",
      "~~~~~~~~~ 12.088025027852119 tf.Tensor([18.208233], shape=(1,), dtype=float32)\n",
      "loss observed: 5.620207786560059\n",
      "action value after update: 18.187095642089844---3\n",
      "______________________________________\n",
      "*********************************************\n",
      "action taken: [3], reward:[35.97646855691082], future reward: [[84.54479]]\n",
      "action value before update: 120.31954193115234\n",
      "~~~~~~~~~ tf.Tensor([120.521255], shape=(1,), dtype=float32) tf.Tensor([120.31954], shape=(1,), dtype=float32)\n",
      "loss observed: 0.02034418098628521\n",
      "action value after update: 120.27943420410156---3\n",
      "______________________________________\n",
      "action taken: [3], reward:[35.78185316280848], future reward: [[51.167713]]\n",
      "action value before update: 86.23965454101562\n",
      "~~~~~~~~~ tf.Tensor([86.94957], shape=(1,), dtype=float32) tf.Tensor([86.239655], shape=(1,), dtype=float32)\n",
      "loss observed: 0.25198978185653687\n",
      "action value after update: 86.303955078125---3\n",
      "______________________________________\n",
      "action taken: [3], reward:[36.26407508355636], future reward: [[17.827148]]\n",
      "action value before update: 52.249122619628906\n",
      "~~~~~~~~~ tf.Tensor([54.091225], shape=(1,), dtype=float32) tf.Tensor([52.249123], shape=(1,), dtype=float32)\n",
      "loss observed: 1.34210205078125\n",
      "action value after update: 52.3318977355957---3\n",
      "______________________________________\n",
      "action taken: [8], reward:[12.186689359462937], future reward: [None]\n",
      "action value before update: -1.6261063814163208\n",
      "~~~~~~~~~ 12.186689359462937 tf.Tensor([-1.6261064], shape=(1,), dtype=float32)\n",
      "loss observed: 13.312795639038086\n",
      "action value after update: -1.5771675109863281---8\n",
      "______________________________________\n",
      "*********************************************\n",
      "action taken: [3], reward:[35.78185316280848], future reward: [[84.83927]]\n",
      "action value before update: 120.72799682617188\n",
      "~~~~~~~~~ tf.Tensor([120.621124], shape=(1,), dtype=float32) tf.Tensor([120.728], shape=(1,), dtype=float32)\n",
      "loss observed: 0.0057108718901872635\n",
      "action value after update: 120.85393524169922---3\n",
      "______________________________________\n",
      "action taken: [3], reward:[36.26407508355636], future reward: [[51.41485]]\n",
      "action value before update: 86.66069793701172\n",
      "~~~~~~~~~ tf.Tensor([87.678925], shape=(1,), dtype=float32) tf.Tensor([86.6607], shape=(1,), dtype=float32)\n",
      "loss observed: 0.5182266235351562\n",
      "action value after update: 86.87167358398438---3\n",
      "______________________________________\n",
      "action taken: [3], reward:[36.56006807838881], future reward: [[17.944221]]\n",
      "action value before update: 52.5921745300293\n",
      "~~~~~~~~~ tf.Tensor([54.504288], shape=(1,), dtype=float32) tf.Tensor([52.592175], shape=(1,), dtype=float32)\n",
      "loss observed: 1.4121131896972656\n",
      "action value after update: 52.7573356628418---3\n",
      "______________________________________\n",
      "action taken: [3], reward:[12.243996892073554], future reward: [None]\n",
      "action value before update: 18.370763778686523\n",
      "~~~~~~~~~ 12.243996892073554 tf.Tensor([18.370764], shape=(1,), dtype=float32)\n",
      "loss observed: 5.626767158508301\n",
      "action value after update: 18.414749145507812---3\n",
      "______________________________________\n",
      "*********************************************\n",
      "action taken: [3], reward:[36.26407508355636], future reward: [[85.61463]]\n",
      "action value before update: 121.83161926269531\n",
      "~~~~~~~~~ tf.Tensor([121.87871], shape=(1,), dtype=float32) tf.Tensor([121.83162], shape=(1,), dtype=float32)\n",
      "loss observed: 0.0011086692102253437\n",
      "action value after update: 122.11511993408203---3\n",
      "______________________________________\n",
      "action taken: [6], reward:[73.12013615677762], future reward: [[18.886435]]\n",
      "action value before update: -8.509298324584961\n",
      "~~~~~~~~~ tf.Tensor([92.00657], shape=(1,), dtype=float32) tf.Tensor([-8.509298], shape=(1,), dtype=float32)\n",
      "loss observed: 100.015869140625\n",
      "action value after update: -7.92041015625---6\n",
      "______________________________________\n",
      "action taken: [3], reward:[12.243996892073554], future reward: [None]\n",
      "action value before update: 19.310361862182617\n",
      "~~~~~~~~~ 12.243996892073554 tf.Tensor([19.310362], shape=(1,), dtype=float32)\n",
      "loss observed: 6.5663652420043945\n",
      "action value after update: 19.30733299255371---3\n",
      "______________________________________\n",
      "*********************************************\n",
      "action taken: [3], reward:[36.56006807838881], future reward: [[86.12278]]\n",
      "action value before update: 122.55994415283203\n",
      "~~~~~~~~~ tf.Tensor([122.682846], shape=(1,), dtype=float32) tf.Tensor([122.559944], shape=(1,), dtype=float32)\n",
      "loss observed: 0.007552440743893385\n",
      "action value after update: 122.76091003417969---3\n",
      "______________________________________\n",
      "action taken: [3], reward:[36.73199067622066], future reward: [[52.2274]]\n",
      "action value before update: 88.02256774902344\n",
      "~~~~~~~~~ tf.Tensor([88.9594], shape=(1,), dtype=float32) tf.Tensor([88.02257], shape=(1,), dtype=float32)\n",
      "loss observed: 0.438823938369751\n",
      "action value after update: 88.27515411376953---3\n",
      "______________________________________\n",
      "action taken: [3], reward:[37.40031819246944], future reward: [[18.218987]]\n",
      "action value before update: 53.44443130493164\n",
      "~~~~~~~~~ tf.Tensor([55.619305], shape=(1,), dtype=float32) tf.Tensor([53.44443], shape=(1,), dtype=float32)\n",
      "loss observed: 1.6748733520507812\n",
      "action value after update: 53.63163375854492---3\n",
      "______________________________________\n",
      "action taken: [3], reward:[12.270348490641275], future reward: [None]\n",
      "action value before update: 18.65178680419922\n",
      "~~~~~~~~~ 12.270348490641275 tf.Tensor([18.651787], shape=(1,), dtype=float32)\n",
      "loss observed: 5.881438255310059\n",
      "action value after update: 18.670269012451172---3\n",
      "______________________________________\n",
      "*********************************************\n",
      "action taken: [3], reward:[36.73199067622066], future reward: [[87.069855]]\n",
      "action value before update: 123.9006576538086\n",
      "~~~~~~~~~ tf.Tensor([123.80185], shape=(1,), dtype=float32) tf.Tensor([123.90066], shape=(1,), dtype=float32)\n",
      "loss observed: 0.004881538916379213\n",
      "action value after update: 124.19189453125---3\n",
      "______________________________________\n",
      "action taken: [3], reward:[37.40031819246944], future reward: [[52.815403]]\n",
      "action value before update: 89.05412292480469\n",
      "~~~~~~~~~ tf.Tensor([90.21572], shape=(1,), dtype=float32) tf.Tensor([89.05412], shape=(1,), dtype=float32)\n",
      "loss observed: 0.6615982055664062\n",
      "action value after update: 89.37525177001953---3\n",
      "______________________________________\n",
      "action taken: [3], reward:[36.811045471923826], future reward: [[18.341825]]\n",
      "action value before update: 54.08645248413086\n",
      "~~~~~~~~~ tf.Tensor([55.15287], shape=(1,), dtype=float32) tf.Tensor([54.086452], shape=(1,), dtype=float32)\n",
      "loss observed: 0.5664176940917969\n",
      "action value after update: 54.312217712402344---3\n",
      "______________________________________\n",
      "action taken: [10], reward:[12.089874984069821], future reward: [None]\n",
      "action value before update: -7.775755882263184\n",
      "~~~~~~~~~ 12.089874984069821 tf.Tensor([-7.775756], shape=(1,), dtype=float32)\n",
      "loss observed: 19.365631103515625\n",
      "action value after update: -7.700130462646484---10\n",
      "______________________________________\n",
      "*********************************************\n",
      "action taken: [3], reward:[37.40031819246944], future reward: [[88.263985]]\n",
      "action value before update: 125.63101959228516\n",
      "~~~~~~~~~ tf.Tensor([125.66431], shape=(1,), dtype=float32) tf.Tensor([125.63102], shape=(1,), dtype=float32)\n",
      "loss observed: 0.0005540137644857168\n",
      "action value after update: 126.05479431152344---3\n",
      "______________________________________\n",
      "action taken: [3], reward:[36.811045471923826], future reward: [[53.58992]]\n",
      "action value before update: 90.3687515258789\n",
      "~~~~~~~~~ tf.Tensor([90.40097], shape=(1,), dtype=float32) tf.Tensor([90.36875], shape=(1,), dtype=float32)\n",
      "loss observed: 0.0005190297961235046\n",
      "action value after update: 90.64752960205078---3\n",
      "______________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "action taken: [3], reward:[36.26962495220946], future reward: [[18.480862]]\n",
      "action value before update: 54.85186767578125\n",
      "~~~~~~~~~ tf.Tensor([54.75049], shape=(1,), dtype=float32) tf.Tensor([54.851868], shape=(1,), dtype=float32)\n",
      "loss observed: 0.005138890817761421\n",
      "action value after update: 54.99861526489258---3\n",
      "______________________________________\n",
      "action taken: [3], reward:[11.761898301740192], future reward: [None]\n",
      "action value before update: 18.883298873901367\n",
      "~~~~~~~~~ 11.761898301740192 tf.Tensor([18.883299], shape=(1,), dtype=float32)\n",
      "loss observed: 6.621400833129883\n",
      "action value after update: 18.883516311645508---3\n",
      "______________________________________\n",
      "*********************************************\n",
      "action taken: [3], reward:[36.811045471923826], future reward: [[89.24853]]\n",
      "action value before update: 127.03962707519531\n",
      "~~~~~~~~~ tf.Tensor([126.05957], shape=(1,), dtype=float32) tf.Tensor([127.03963], shape=(1,), dtype=float32)\n",
      "loss observed: 0.4802556335926056\n",
      "action value after update: 127.0141372680664---3\n",
      "______________________________________\n",
      "action taken: [3], reward:[36.26962495220946], future reward: [[53.97595]]\n",
      "action value before update: 91.04995727539062\n",
      "~~~~~~~~~ tf.Tensor([90.245575], shape=(1,), dtype=float32) tf.Tensor([91.04996], shape=(1,), dtype=float32)\n",
      "loss observed: 0.32351547479629517\n",
      "action value after update: 90.9209976196289---3\n",
      "______________________________________\n",
      "action taken: [3], reward:[35.28569490522058], future reward: [[18.42033]]\n",
      "action value before update: 54.996788024902344\n",
      "~~~~~~~~~ tf.Tensor([53.706024], shape=(1,), dtype=float32) tf.Tensor([54.996788], shape=(1,), dtype=float32)\n",
      "loss observed: 0.7907638549804688\n",
      "action value after update: 54.87188720703125---3\n",
      "______________________________________\n",
      "action taken: [3], reward:[12.119022072033182], future reward: [None]\n",
      "action value before update: 18.720571517944336\n",
      "~~~~~~~~~ 12.119022072033182 tf.Tensor([18.720572], shape=(1,), dtype=float32)\n",
      "loss observed: 6.10154914855957\n",
      "action value after update: 18.629600524902344---3\n",
      "______________________________________\n",
      "*********************************************\n",
      "action taken: [3], reward:[36.26962495220946], future reward: [[88.68243]]\n",
      "action value before update: 126.26384735107422\n",
      "~~~~~~~~~ tf.Tensor([124.95206], shape=(1,), dtype=float32) tf.Tensor([126.26385], shape=(1,), dtype=float32)\n",
      "loss observed: 0.8117904663085938\n",
      "action value after update: 125.74436950683594---3\n",
      "______________________________________\n",
      "action taken: [3], reward:[35.28569490522058], future reward: [[53.43562]]\n",
      "action value before update: 90.11634826660156\n",
      "~~~~~~~~~ tf.Tensor([88.72131], shape=(1,), dtype=float32) tf.Tensor([90.11635], shape=(1,), dtype=float32)\n",
      "loss observed: 0.8950347900390625\n",
      "action value after update: 89.64546203613281---3\n",
      "______________________________________\n",
      "action taken: [3], reward:[36.35706621609955], future reward: [[18.001226]]\n",
      "action value before update: 54.235877990722656\n",
      "~~~~~~~~~ tf.Tensor([54.35829], shape=(1,), dtype=float32) tf.Tensor([54.235878], shape=(1,), dtype=float32)\n",
      "loss observed: 0.007492549251765013\n",
      "action value after update: 53.980857849121094---3\n",
      "______________________________________\n",
      "action taken: [3], reward:[12.095507072999272], future reward: [None]\n",
      "action value before update: 18.24576187133789\n",
      "~~~~~~~~~ 12.095507072999272 tf.Tensor([18.245762], shape=(1,), dtype=float32)\n",
      "loss observed: 5.65025520324707\n",
      "action value after update: 18.112403869628906---3\n",
      "______________________________________\n",
      "*********************************************\n",
      "action taken: [3], reward:[35.28569490522058], future reward: [[87.0721]]\n",
      "action value before update: 123.94742584228516\n",
      "~~~~~~~~~ tf.Tensor([122.35779], shape=(1,), dtype=float32) tf.Tensor([123.947426], shape=(1,), dtype=float32)\n",
      "loss observed: 1.0896377563476562\n",
      "action value after update: 123.19862365722656---3\n",
      "______________________________________\n",
      "action taken: [3], reward:[36.35706621609955], future reward: [[52.329575]]\n",
      "action value before update: 88.30823516845703\n",
      "~~~~~~~~~ tf.Tensor([88.686646], shape=(1,), dtype=float32) tf.Tensor([88.308235], shape=(1,), dtype=float32)\n",
      "loss observed: 0.07159719616174698\n",
      "action value after update: 87.87176513671875---3\n",
      "______________________________________\n",
      "action taken: [3], reward:[36.28652121899782], future reward: [[17.457756]]\n",
      "action value before update: 53.12879180908203\n",
      "~~~~~~~~~ tf.Tensor([53.744278], shape=(1,), dtype=float32) tf.Tensor([53.12879], shape=(1,), dtype=float32)\n",
      "loss observed: 0.18941159546375275\n",
      "action value after update: 52.917755126953125---3\n",
      "______________________________________\n",
      "action taken: [3], reward:[11.822987967062558], future reward: [None]\n",
      "action value before update: 17.707435607910156\n",
      "~~~~~~~~~ 11.822987967062558 tf.Tensor([17.707436], shape=(1,), dtype=float32)\n",
      "loss observed: 5.384448051452637\n",
      "action value after update: 17.58852767944336---3\n",
      "______________________________________\n",
      "*********************************************\n",
      "action taken: [3], reward:[36.35706621609955], future reward: [[85.4456]]\n",
      "action value before update: 121.671630859375\n",
      "~~~~~~~~~ tf.Tensor([121.80267], shape=(1,), dtype=float32) tf.Tensor([121.67163], shape=(1,), dtype=float32)\n",
      "loss observed: 0.008586065843701363\n",
      "action value after update: 121.28506469726562---3\n",
      "______________________________________\n",
      "action taken: [3], reward:[36.28652121899782], future reward: [[51.469677]]\n",
      "action value before update: 86.9090347290039\n",
      "~~~~~~~~~ tf.Tensor([87.756195], shape=(1,), dtype=float32) tf.Tensor([86.909035], shape=(1,), dtype=float32)\n",
      "loss observed: 0.35884031653404236\n",
      "action value after update: 86.76779174804688---3\n",
      "______________________________________\n",
      "action taken: [3], reward:[35.46896390118767], future reward: [[17.066547]]\n",
      "action value before update: 52.4311408996582\n",
      "~~~~~~~~~ tf.Tensor([52.53551], shape=(1,), dtype=float32) tf.Tensor([52.43114], shape=(1,), dtype=float32)\n",
      "loss observed: 0.0054465606808662415\n",
      "action value after update: 52.35611343383789---3\n",
      "______________________________________\n",
      "action taken: [3], reward:[11.748743057525417], future reward: [None]\n",
      "action value before update: 17.354568481445312\n",
      "~~~~~~~~~ 11.748743057525417 tf.Tensor([17.354568], shape=(1,), dtype=float32)\n",
      "loss observed: 5.105825424194336\n",
      "action value after update: 17.276887893676758---3\n",
      "______________________________________\n",
      "*********************************************\n",
      "action taken: [3], reward:[36.28652121899782], future reward: [[84.769]]\n",
      "action value before update: 120.74270629882812\n",
      "~~~~~~~~~ tf.Tensor([121.05552], shape=(1,), dtype=float32) tf.Tensor([120.74271], shape=(1,), dtype=float32)\n",
      "loss observed: 0.04892592504620552\n",
      "action value after update: 120.65563201904297---3\n",
      "______________________________________\n",
      "action taken: [3], reward:[35.46896390118767], future reward: [[51.17701]]\n",
      "action value before update: 86.4334487915039\n",
      "~~~~~~~~~ tf.Tensor([86.64597], shape=(1,), dtype=float32) tf.Tensor([86.43345], shape=(1,), dtype=float32)\n",
      "loss observed: 0.022583313286304474\n",
      "action value after update: 86.4021224975586---3\n",
      "______________________________________\n",
      "action taken: [3], reward:[35.24622917257625], future reward: [[16.829187]]\n",
      "action value before update: 52.198692321777344\n",
      "~~~~~~~~~ tf.Tensor([52.075417], shape=(1,), dtype=float32) tf.Tensor([52.198692], shape=(1,), dtype=float32)\n",
      "loss observed: 0.007598455995321274\n",
      "action value after update: 52.17204284667969---3\n",
      "______________________________________\n",
      "action taken: [3], reward:[11.821096900706682], future reward: [None]\n",
      "action value before update: 17.12704849243164\n",
      "~~~~~~~~~ 11.821096900706682 tf.Tensor([17.127048], shape=(1,), dtype=float32)\n",
      "loss observed: 4.805951118469238\n",
      "action value after update: 17.061994552612305---3\n",
      "______________________________________\n",
      "*********************************************\n",
      "action taken: [3], reward:[35.46896390118767], future reward: [[84.57138]]\n",
      "action value before update: 120.46973419189453\n",
      "~~~~~~~~~ tf.Tensor([120.040344], shape=(1,), dtype=float32) tf.Tensor([120.469734], shape=(1,), dtype=float32)\n",
      "loss observed: 0.09218786656856537\n",
      "action value after update: 120.28627014160156---3\n",
      "______________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "action taken: [3], reward:[35.24622917257625], future reward: [[51.005123]]\n",
      "action value before update: 86.1624526977539\n",
      "~~~~~~~~~ tf.Tensor([86.25136], shape=(1,), dtype=float32) tf.Tensor([86.16245], shape=(1,), dtype=float32)\n",
      "loss observed: 0.003952079452574253\n",
      "action value after update: 86.052490234375---3\n",
      "______________________________________\n",
      "action taken: [3], reward:[35.46329070212005], future reward: [[16.587927]]\n",
      "action value before update: 51.975311279296875\n",
      "~~~~~~~~~ tf.Tensor([52.051216], shape=(1,), dtype=float32) tf.Tensor([51.97531], shape=(1,), dtype=float32)\n",
      "loss observed: 0.0028807728085666895\n",
      "action value after update: 51.91530227661133---3\n",
      "______________________________________\n",
      "action taken: [3], reward:[12.043831629318106], future reward: [None]\n",
      "action value before update: 16.86800193786621\n",
      "~~~~~~~~~ 12.043831629318106 tf.Tensor([16.868002], shape=(1,), dtype=float32)\n",
      "loss observed: 4.324170112609863\n",
      "action value after update: 16.790695190429688---3\n",
      "______________________________________\n",
      "*********************************************\n",
      "action taken: [3], reward:[35.24622917257625], future reward: [[84.133484]]\n",
      "action value before update: 119.85552978515625\n",
      "~~~~~~~~~ tf.Tensor([119.379715], shape=(1,), dtype=float32) tf.Tensor([119.85553], shape=(1,), dtype=float32)\n",
      "loss observed: 0.1131998747587204\n",
      "action value after update: 119.59789276123047---3\n",
      "______________________________________\n",
      "action taken: [3], reward:[35.46329070212005], future reward: [[50.699657]]\n",
      "action value before update: 85.6623764038086\n",
      "~~~~~~~~~ tf.Tensor([86.16295], shape=(1,), dtype=float32) tf.Tensor([85.66238], shape=(1,), dtype=float32)\n",
      "loss observed: 0.125286266207695\n",
      "action value after update: 85.55872344970703---3\n",
      "______________________________________\n",
      "action taken: [3], reward:[36.13149488795432], future reward: [[16.301249]]\n",
      "action value before update: 51.66759490966797\n",
      "~~~~~~~~~ tf.Tensor([52.432747], shape=(1,), dtype=float32) tf.Tensor([51.667595], shape=(1,), dtype=float32)\n",
      "loss observed: 0.2927287817001343\n",
      "action value after update: 51.64554977416992---3\n",
      "______________________________________\n",
      "action taken: [3], reward:[11.92111786687715], future reward: [None]\n",
      "action value before update: 16.58928108215332\n",
      "~~~~~~~~~ 11.92111786687715 tf.Tensor([16.589281], shape=(1,), dtype=float32)\n",
      "loss observed: 4.168163299560547\n",
      "action value after update: 16.52379608154297---3\n",
      "______________________________________\n",
      "*********************************************\n",
      "action taken: [3], reward:[35.46329070212005], future reward: [[83.77035]]\n",
      "action value before update: 119.34261322021484\n",
      "~~~~~~~~~ tf.Tensor([119.23364], shape=(1,), dtype=float32) tf.Tensor([119.34261], shape=(1,), dtype=float32)\n",
      "loss observed: 0.005937300622463226\n",
      "action value after update: 119.24622344970703---3\n",
      "______________________________________\n",
      "action taken: [3], reward:[36.13149488795432], future reward: [[50.521725]]\n",
      "action value before update: 85.40798950195312\n",
      "~~~~~~~~~ tf.Tensor([86.65322], shape=(1,), dtype=float32) tf.Tensor([85.40799], shape=(1,), dtype=float32)\n",
      "loss observed: 0.7452316284179688\n",
      "action value after update: 85.47464752197266---3\n",
      "______________________________________\n",
      "action taken: [3], reward:[35.763353600631454], future reward: [[16.101078]]\n",
      "action value before update: 51.59014129638672\n",
      "~~~~~~~~~ tf.Tensor([51.864433], shape=(1,), dtype=float32) tf.Tensor([51.59014], shape=(1,), dtype=float32)\n",
      "loss observed: 0.03761804848909378\n",
      "action value after update: 51.63753128051758---3\n",
      "______________________________________\n",
      "action taken: [3], reward:[11.878198882626446], future reward: [None]\n",
      "action value before update: 16.408123016357422\n",
      "~~~~~~~~~ 11.878198882626446 tf.Tensor([16.408123], shape=(1,), dtype=float32)\n",
      "loss observed: 4.029924392700195\n",
      "action value after update: 16.362581253051758---3\n",
      "______________________________________\n",
      "*********************************************\n",
      "action taken: [3], reward:[36.13149488795432], future reward: [[83.88427]]\n",
      "action value before update: 119.53758239746094\n",
      "~~~~~~~~~ tf.Tensor([120.01576], shape=(1,), dtype=float32) tf.Tensor([119.53758], shape=(1,), dtype=float32)\n",
      "loss observed: 0.11432802677154541\n",
      "action value after update: 119.7209701538086---3\n",
      "______________________________________\n",
      "action taken: [3], reward:[35.763353600631454], future reward: [[50.649475]]\n",
      "action value before update: 85.7248764038086\n",
      "~~~~~~~~~ tf.Tensor([86.41283], shape=(1,), dtype=float32) tf.Tensor([85.72488], shape=(1,), dtype=float32)\n",
      "loss observed: 0.23663769662380219\n",
      "action value after update: 85.93172454833984---3\n",
      "______________________________________\n",
      "action taken: [3], reward:[35.634596647879334], future reward: [[16.010492]]\n",
      "action value before update: 51.729915618896484\n",
      "~~~~~~~~~ tf.Tensor([51.64509], shape=(1,), dtype=float32) tf.Tensor([51.729916], shape=(1,), dtype=float32)\n",
      "loss observed: 0.0035978457890450954\n",
      "action value after update: 51.75761413574219---3\n",
      "______________________________________\n",
      "action taken: [3], reward:[11.891888558637447], future reward: [None]\n",
      "action value before update: 16.33041000366211\n",
      "~~~~~~~~~ 11.891888558637447 tf.Tensor([16.33041], shape=(1,), dtype=float32)\n",
      "loss observed: 3.938521385192871\n",
      "action value after update: 16.296937942504883---3\n",
      "______________________________________\n",
      "*********************************************\n",
      "action taken: [3], reward:[35.763353600631454], future reward: [[84.51809]]\n",
      "action value before update: 120.45142364501953\n",
      "~~~~~~~~~ tf.Tensor([120.28145], shape=(1,), dtype=float32) tf.Tensor([120.45142], shape=(1,), dtype=float32)\n",
      "loss observed: 0.014445798471570015\n",
      "action value after update: 120.57939147949219---3\n",
      "______________________________________\n",
      "action taken: [3], reward:[35.634596647879334], future reward: [[50.653194]]\n",
      "action value before update: 86.33158111572266\n",
      "~~~~~~~~~ tf.Tensor([86.287796], shape=(1,), dtype=float32) tf.Tensor([86.33158], shape=(1,), dtype=float32)\n",
      "loss observed: 0.0009585672523826361\n",
      "action value after update: 86.40562438964844---3\n",
      "______________________________________\n",
      "action taken: [3], reward:[35.67566567591234], future reward: [[15.904332]]\n",
      "action value before update: 51.64185333251953\n",
      "~~~~~~~~~ tf.Tensor([51.579998], shape=(1,), dtype=float32) tf.Tensor([51.641853], shape=(1,), dtype=float32)\n",
      "loss observed: 0.001913040061481297\n",
      "action value after update: 51.5898551940918---3\n",
      "______________________________________\n",
      "action taken: [3], reward:[12.169340881154703], future reward: [None]\n",
      "action value before update: 16.19231605529785\n",
      "~~~~~~~~~ 12.169340881154703 tf.Tensor([16.192316], shape=(1,), dtype=float32)\n",
      "loss observed: 3.522974967956543\n",
      "action value after update: 16.1297664642334---3\n",
      "______________________________________\n",
      "*********************************************\n",
      "action taken: [3], reward:[35.634596647879334], future reward: [[84.763985]]\n",
      "action value before update: 120.81599426269531\n",
      "~~~~~~~~~ tf.Tensor([120.39858], shape=(1,), dtype=float32) tf.Tensor([120.815994], shape=(1,), dtype=float32)\n",
      "loss observed: 0.0871163085103035\n",
      "action value after update: 120.74365997314453---3\n",
      "______________________________________\n",
      "action taken: [3], reward:[35.67566567591234], future reward: [[50.325645]]\n",
      "action value before update: 86.43834686279297\n",
      "~~~~~~~~~ tf.Tensor([86.00131], shape=(1,), dtype=float32) tf.Tensor([86.43835], shape=(1,), dtype=float32)\n",
      "loss observed: 0.09549962729215622\n",
      "action value after update: 86.32891845703125---3\n",
      "______________________________________\n",
      "action taken: [3], reward:[36.50802264346411], future reward: [[15.65521]]\n",
      "action value before update: 51.18761444091797\n",
      "~~~~~~~~~ tf.Tensor([52.16323], shape=(1,), dtype=float32) tf.Tensor([51.187614], shape=(1,), dtype=float32)\n",
      "loss observed: 0.47591373324394226\n",
      "action value after update: 51.19084548950195---3\n",
      "______________________________________\n",
      "action taken: [3], reward:[12.152362394090009], future reward: [None]\n",
      "action value before update: 15.95865249633789\n",
      "~~~~~~~~~ 12.152362394090009 tf.Tensor([15.9586525], shape=(1,), dtype=float32)\n",
      "loss observed: 3.3062896728515625\n",
      "action value after update: 15.914152145385742---3\n",
      "______________________________________\n",
      "*********************************************\n",
      "action taken: [3], reward:[35.67566567591234], future reward: [[84.55315]]\n",
      "action value before update: 120.51583099365234\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "~~~~~~~~~ tf.Tensor([120.22881], shape=(1,), dtype=float32) tf.Tensor([120.51583], shape=(1,), dtype=float32)\n",
      "loss observed: 0.041189614683389664\n",
      "action value after update: 120.38671875---3\n",
      "______________________________________\n",
      "action taken: [3], reward:[36.50802264346411], future reward: [[50.046]]\n",
      "action value before update: 86.1828384399414\n",
      "~~~~~~~~~ tf.Tensor([86.55402], shape=(1,), dtype=float32) tf.Tensor([86.18284], shape=(1,), dtype=float32)\n",
      "loss observed: 0.06888926774263382\n",
      "action value after update: 86.14693450927734---3\n",
      "______________________________________\n",
      "action taken: [3], reward:[36.45708718227002], future reward: [[15.51204]]\n",
      "action value before update: 51.029701232910156\n",
      "~~~~~~~~~ tf.Tensor([51.969128], shape=(1,), dtype=float32) tf.Tensor([51.0297], shape=(1,), dtype=float32)\n",
      "loss observed: 0.4412609934806824\n",
      "action value after update: 51.108497619628906---3\n",
      "______________________________________\n",
      "action taken: [3], reward:[12.286340334389863], future reward: [None]\n",
      "action value before update: 15.842714309692383\n",
      "~~~~~~~~~ 12.286340334389863 tf.Tensor([15.842714], shape=(1,), dtype=float32)\n",
      "loss observed: 3.0563735961914062\n",
      "action value after update: 15.825413703918457---3\n",
      "______________________________________\n",
      "*********************************************\n",
      "action taken: [3], reward:[36.50802264346411], future reward: [[84.47946]]\n",
      "action value before update: 120.43995666503906\n",
      "~~~~~~~~~ tf.Tensor([120.98749], shape=(1,), dtype=float32) tf.Tensor([120.43996], shape=(1,), dtype=float32)\n",
      "loss observed: 0.14989516139030457\n",
      "action value after update: 120.60246276855469---3\n",
      "______________________________________\n",
      "action taken: [3], reward:[36.45708718227002], future reward: [[50.19314]]\n",
      "action value before update: 86.31724548339844\n",
      "~~~~~~~~~ tf.Tensor([86.65022], shape=(1,), dtype=float32) tf.Tensor([86.317245], shape=(1,), dtype=float32)\n",
      "loss observed: 0.055436939001083374\n",
      "action value after update: 86.46540069580078---3\n",
      "______________________________________\n",
      "action taken: [3], reward:[36.85902100316959], future reward: [[15.530569]]\n",
      "action value before update: 51.321258544921875\n",
      "~~~~~~~~~ tf.Tensor([52.389587], shape=(1,), dtype=float32) tf.Tensor([51.32126], shape=(1,), dtype=float32)\n",
      "loss observed: 0.568328857421875\n",
      "action value after update: 51.52399444580078---3\n",
      "______________________________________\n",
      "action taken: [3], reward:[12.27503504639279], future reward: [None]\n",
      "action value before update: 15.907800674438477\n",
      "~~~~~~~~~ 12.27503504639279 tf.Tensor([15.907801], shape=(1,), dtype=float32)\n",
      "loss observed: 3.132765769958496\n",
      "action value after update: 15.932923316955566---3\n",
      "______________________________________\n",
      "*********************************************\n",
      "action taken: [0], reward:[0.0], future reward: [[118.17669]]\n",
      "action value before update: -51.196510314941406\n",
      "~~~~~~~~~ tf.Tensor([118.17669], shape=(1,), dtype=float32) tf.Tensor([-51.19651], shape=(1,), dtype=float32)\n",
      "loss observed: 168.87319946289062\n",
      "action value after update: -50.31736373901367---0\n",
      "______________________________________\n",
      "action taken: [3], reward:[36.85902100316959], future reward: [[84.46904]]\n",
      "action value before update: 120.74150085449219\n",
      "~~~~~~~~~ tf.Tensor([121.328064], shape=(1,), dtype=float32) tf.Tensor([120.7415], shape=(1,), dtype=float32)\n",
      "loss observed: 0.17202813923358917\n",
      "action value after update: 121.03404998779297---3\n",
      "______________________________________\n",
      "action taken: [3], reward:[36.82510513917837], future reward: [[50.378677]]\n",
      "action value before update: 86.40007781982422\n",
      "~~~~~~~~~ tf.Tensor([87.20378], shape=(1,), dtype=float32) tf.Tensor([86.40008], shape=(1,), dtype=float32)\n",
      "loss observed: 0.3229694962501526\n",
      "action value after update: 86.69528198242188---3\n",
      "______________________________________\n",
      "action taken: [3], reward:[36.82226853964456], future reward: [[15.223472]]\n",
      "action value before update: 51.61760330200195\n",
      "~~~~~~~~~ tf.Tensor([52.045742], shape=(1,), dtype=float32) tf.Tensor([51.617603], shape=(1,), dtype=float32)\n",
      "loss observed: 0.09165138751268387\n",
      "action value after update: 51.8537483215332---3\n",
      "______________________________________\n",
      "action taken: [0], reward:[0.0], future reward: [[14.784977]]\n",
      "action value before update: -8.201017379760742\n",
      "~~~~~~~~~ tf.Tensor([14.784977], shape=(1,), dtype=float32) tf.Tensor([-8.201017], shape=(1,), dtype=float32)\n",
      "loss observed: 22.485994338989258\n",
      "action value after update: -8.088725090026855---0\n",
      "______________________________________\n",
      "action taken: [3], reward:[12.508129529823348], future reward: [None]\n",
      "action value before update: 15.150568962097168\n",
      "~~~~~~~~~ 12.508129529823348 tf.Tensor([15.150569], shape=(1,), dtype=float32)\n",
      "loss observed: 2.142439842224121\n",
      "action value after update: 15.186849594116211---3\n",
      "______________________________________\n",
      "*********************************************\n",
      "action taken: [3], reward:[36.85902100316959], future reward: [[86.48965]]\n",
      "action value before update: 123.32688903808594\n",
      "~~~~~~~~~ tf.Tensor([123.34866], shape=(1,), dtype=float32) tf.Tensor([123.32689], shape=(1,), dtype=float32)\n",
      "loss observed: 0.00023705989588052034\n",
      "action value after update: 123.60612487792969---3\n",
      "______________________________________\n",
      "action taken: [3], reward:[36.82510513917837], future reward: [[51.824482]]\n",
      "action value before update: 88.45234680175781\n",
      "~~~~~~~~~ tf.Tensor([88.64958], shape=(1,), dtype=float32) tf.Tensor([88.45235], shape=(1,), dtype=float32)\n",
      "loss observed: 0.019450843334197998\n",
      "action value after update: 88.65856170654297---3\n",
      "______________________________________\n",
      "action taken: [3], reward:[36.82226853964456], future reward: [[15.96974]]\n",
      "action value before update: 53.02641677856445\n",
      "~~~~~~~~~ tf.Tensor([52.792007], shape=(1,), dtype=float32) tf.Tensor([53.026417], shape=(1,), dtype=float32)\n",
      "loss observed: 0.027473866939544678\n",
      "action value after update: 53.13004684448242---3\n",
      "______________________________________\n",
      "action taken: [3], reward:[12.399598765051449], future reward: [None]\n",
      "action value before update: 16.31780242919922\n",
      "~~~~~~~~~ 12.399598765051449 tf.Tensor([16.317802], shape=(1,), dtype=float32)\n",
      "loss observed: 3.418203353881836\n",
      "action value after update: 16.311025619506836---3\n",
      "______________________________________\n",
      "*********************************************\n",
      "action taken: [3], reward:[36.82510513917837], future reward: [[87.16392]]\n",
      "action value before update: 124.30140686035156\n",
      "~~~~~~~~~ tf.Tensor([123.98902], shape=(1,), dtype=float32) tf.Tensor([124.30141], shape=(1,), dtype=float32)\n",
      "loss observed: 0.04879236966371536\n",
      "action value after update: 124.36859130859375---3\n",
      "______________________________________\n",
      "action taken: [3], reward:[36.82226853964456], future reward: [[52.125267]]\n",
      "action value before update: 88.9875259399414\n",
      "~~~~~~~~~ tf.Tensor([88.94754], shape=(1,), dtype=float32) tf.Tensor([88.987526], shape=(1,), dtype=float32)\n",
      "loss observed: 0.0007994263432919979\n",
      "action value after update: 89.02216339111328---3\n",
      "______________________________________\n",
      "action taken: [3], reward:[37.198796295154345], future reward: [[15.949044]]\n",
      "action value before update: 53.191978454589844\n",
      "~~~~~~~~~ tf.Tensor([53.14784], shape=(1,), dtype=float32) tf.Tensor([53.19198], shape=(1,), dtype=float32)\n",
      "loss observed: 0.0009741637040860951\n",
      "action value after update: 53.18938064575195---3\n",
      "______________________________________\n",
      "action taken: [3], reward:[12.508129529823348], future reward: [None]\n",
      "action value before update: 16.255474090576172\n",
      "~~~~~~~~~ 12.508129529823348 tf.Tensor([16.255474], shape=(1,), dtype=float32)\n",
      "loss observed: 3.247344970703125\n",
      "action value after update: 16.210433959960938---3\n",
      "______________________________________\n",
      "*********************************************\n",
      "action taken: [3], reward:[36.82226853964456], future reward: [[87.26735]]\n",
      "action value before update: 124.45945739746094\n",
      "~~~~~~~~~ tf.Tensor([124.089615], shape=(1,), dtype=float32) tf.Tensor([124.45946], shape=(1,), dtype=float32)\n",
      "loss observed: 0.06839174777269363\n",
      "action value after update: 124.3526611328125---3\n",
      "______________________________________\n",
      "action taken: [3], reward:[37.198796295154345], future reward: [[51.993954]]\n",
      "action value before update: 88.9677505493164\n",
      "~~~~~~~~~ tf.Tensor([89.19275], shape=(1,), dtype=float32) tf.Tensor([88.96775], shape=(1,), dtype=float32)\n",
      "loss observed: 0.025312157347798347\n",
      "action value after update: 88.92737579345703---3\n",
      "______________________________________\n",
      "action taken: [3], reward:[37.52438858947004], future reward: [[15.7841425]]\n",
      "action value before update: 52.99485778808594\n",
      "~~~~~~~~~ tf.Tensor([53.30853], shape=(1,), dtype=float32) tf.Tensor([52.994858], shape=(1,), dtype=float32)\n",
      "loss observed: 0.049194782972335815\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "action value after update: 52.977745056152344---3\n",
      "______________________________________\n",
      "action taken: [3], reward:[12.232568273661968], future reward: [None]\n",
      "action value before update: 16.081174850463867\n",
      "~~~~~~~~~ 12.232568273661968 tf.Tensor([16.081175], shape=(1,), dtype=float32)\n",
      "loss observed: 3.3486061096191406\n",
      "action value after update: 16.028844833374023---3\n",
      "______________________________________\n",
      "*********************************************\n",
      "action taken: [3], reward:[37.198796295154345], future reward: [[87.10585]]\n",
      "action value before update: 124.24908447265625\n",
      "~~~~~~~~~ tf.Tensor([124.30464], shape=(1,), dtype=float32) tf.Tensor([124.249084], shape=(1,), dtype=float32)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-61-8a88170cbf24>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     41\u001b[0m             \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"~~~~~~~~~\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mG\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcurrent_predict\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     42\u001b[0m             \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mloss_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mG\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcurrent_predict\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 43\u001b[1;33m             \u001b[0mgrad\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtape\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgradient\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmain_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrainable_variables\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     44\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply_gradients\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgrad\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmain_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrainable_variables\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     45\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf\"loss observed: {loss}\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\DevelopmentTools\\envs\\quant\\lib\\site-packages\\tensorflow\\python\\eager\\backprop.py\u001b[0m in \u001b[0;36mgradient\u001b[1;34m(self, target, sources, output_gradients, unconnected_gradients)\u001b[0m\n\u001b[0;32m   1065\u001b[0m                           for x in nest.flatten(output_gradients)]\n\u001b[0;32m   1066\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1067\u001b[1;33m     flat_grad = imperative_grad.imperative_grad(\n\u001b[0m\u001b[0;32m   1068\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_tape\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1069\u001b[0m         \u001b[0mflat_targets\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\DevelopmentTools\\envs\\quant\\lib\\site-packages\\tensorflow\\python\\eager\\imperative_grad.py\u001b[0m in \u001b[0;36mimperative_grad\u001b[1;34m(tape, target, sources, output_gradients, sources_raw, unconnected_gradients)\u001b[0m\n\u001b[0;32m     69\u001b[0m         \"Unknown value for unconnected_gradients: %r\" % unconnected_gradients)\n\u001b[0;32m     70\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 71\u001b[1;33m   return pywrap_tfe.TFE_Py_TapeGradient(\n\u001b[0m\u001b[0;32m     72\u001b[0m       \u001b[0mtape\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_tape\u001b[0m\u001b[1;33m,\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     73\u001b[0m       \u001b[0mtarget\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\DevelopmentTools\\envs\\quant\\lib\\site-packages\\tensorflow\\python\\eager\\backprop.py\u001b[0m in \u001b[0;36m_gradient_function\u001b[1;34m(op_name, attr_tuple, num_inputs, inputs, outputs, out_grads, skip_input_indices, forward_pass_name_scope)\u001b[0m\n\u001b[0;32m    160\u001b[0m       \u001b[0mgradient_name_scope\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mforward_pass_name_scope\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m\"/\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    161\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname_scope\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgradient_name_scope\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 162\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mgrad_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmock_op\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0mout_grads\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    163\u001b[0m   \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    164\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mgrad_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmock_op\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0mout_grads\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\DevelopmentTools\\envs\\quant\\lib\\site-packages\\tensorflow\\python\\ops\\math_grad.py\u001b[0m in \u001b[0;36m_DivNoNanGrad\u001b[1;34m(op, grad)\u001b[0m\n\u001b[0;32m   1427\u001b[0m   \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mop\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1428\u001b[0m   \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mop\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1429\u001b[1;33m   \u001b[0msx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0marray_ops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1430\u001b[0m   \u001b[0msy\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0marray_ops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1431\u001b[0m   \u001b[0mrx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mry\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgen_array_ops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbroadcast_gradient_args\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msy\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\DevelopmentTools\\envs\\quant\\lib\\site-packages\\tensorflow\\python\\util\\dispatch.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    199\u001b[0m     \u001b[1;34m\"\"\"Call target, and fall back on dispatchers if there is a TypeError.\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    200\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 201\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mtarget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    202\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mTypeError\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    203\u001b[0m       \u001b[1;31m# Note: convert_to_eager_tensor currently raises a ValueError, not a\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\DevelopmentTools\\envs\\quant\\lib\\site-packages\\tensorflow\\python\\ops\\array_ops.py\u001b[0m in \u001b[0;36mshape\u001b[1;34m(input, name, out_type)\u001b[0m\n\u001b[0;32m    626\u001b[0m     \u001b[0mA\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;31m`\u001b[0m \u001b[0mof\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mout_type\u001b[0m\u001b[0;31m`\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    627\u001b[0m   \"\"\"\n\u001b[1;32m--> 628\u001b[1;33m   \u001b[1;32mreturn\u001b[0m \u001b[0mshape_internal\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptimize\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mout_type\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mout_type\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    629\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    630\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\DevelopmentTools\\envs\\quant\\lib\\site-packages\\tensorflow\\python\\ops\\array_ops.py\u001b[0m in \u001b[0;36mshape_internal\u001b[1;34m(input, name, optimize, out_type)\u001b[0m\n\u001b[0;32m    654\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0moptimize\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0minput_shape\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_fully_defined\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    655\u001b[0m           \u001b[1;32mreturn\u001b[0m \u001b[0mconstant\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput_shape\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_list\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mout_type\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 656\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mgen_array_ops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mout_type\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mout_type\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    657\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    658\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\DevelopmentTools\\envs\\quant\\lib\\site-packages\\tensorflow\\python\\ops\\gen_array_ops.py\u001b[0m in \u001b[0;36mshape\u001b[1;34m(input, out_type, name)\u001b[0m\n\u001b[0;32m   9015\u001b[0m   \u001b[1;32mif\u001b[0m \u001b[0mtld\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_eager\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   9016\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 9017\u001b[1;33m       _result = pywrap_tfe.TFE_Py_FastPathExecute(\n\u001b[0m\u001b[0;32m   9018\u001b[0m         \u001b[0m_ctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_context_handle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtld\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdevice_name\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"Shape\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   9019\u001b[0m         tld.op_callbacks, input, \"out_type\", out_type)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "np.random.seed(777)\n",
    "for ep in range(num_episode):\n",
    "\n",
    "    \n",
    "    env = trade_env(data = train_data[:,:,ep+1:])\n",
    "    current_state = env.start()\n",
    "    while True:\n",
    "    \n",
    "        \n",
    "        z = np.random.uniform()\n",
    "        if z<=0.9:\n",
    "            action = tf.argmax(main_model(current_state),axis=1).numpy()[0]\n",
    "        else:\n",
    "            action = np.random.choice(11)\n",
    "            \n",
    "        next_state, reward, end_episode = env.step(action)\n",
    "        masks = tf.one_hot(action, 11)\n",
    "        \n",
    "#         print(current_state[1],next_state[1])\n",
    "        future_reward = None\n",
    "        \n",
    "        if end_episode:\n",
    "            G = reward\n",
    "        else:\n",
    "            future_reward = gamma*tf.reduce_max(main_model.predict(next_state),axis=1)\n",
    "            G = reward + future_reward # todo: should use target\n",
    "            \n",
    "        print(f\"action taken: [{action}], reward:[{reward}], future reward: [{future_reward}]\" )\n",
    "        print(f\"action value before update: {main_model.predict(current_state)[0][action]}\")\n",
    "        \n",
    "            \n",
    "            \n",
    "        with tf.GradientTape() as tape:\n",
    "            \n",
    "#             tape.watch(main_model.trainable_variables)\n",
    "            current_state_action_val = main_model(current_state)\n",
    "            current_predict = tf.reduce_sum(tf.multiply(current_state_action_val,masks),axis=1)\n",
    "    \n",
    "    \n",
    "#             current_predict = tf.reduce_max(current_state_action_val,axis=1)\n",
    "            print(\"~~~~~~~~~\",G, current_predict)\n",
    "            loss = loss_function(G, current_predict)\n",
    "            grad = tape.gradient(loss, main_model.trainable_variables)\n",
    "            optimizer.apply_gradients(zip(grad, main_model.trainable_variables))\n",
    "        print(f\"loss observed: {loss}\")\n",
    "#     \n",
    "        print(f\"action value after update: {main_model.predict(current_state)[0][action]}---{action}\")   \n",
    " \n",
    "#         if main_model.predict(current_state)[0][action]==0:\n",
    "#             print(grad)\n",
    "        print(\"______________________________________\")\n",
    "            \n",
    "        current_state = next_state\n",
    "        \n",
    "        if end_episode:\n",
    "            break\n",
    "    print(\"*********************************************\")\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da158374",
   "metadata": {},
   "source": [
    "# test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfcd66e7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bf518d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "## genearte fake inputs \n",
    "from scipy.stats import norm\n",
    "sample_size, time_lenght, ts_feature_num = 1000,60,5\n",
    "current_feature_num = 3\n",
    "\n",
    "time_series_data = norm.rvs( size = [sample_size, time_lenght, ts_feature_num])\n",
    "current_data = norm.rvs(size=[sample_size, current_feature_num])\n",
    "target_data = norm.rvs(size = [sample_size,10])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5acbf94a",
   "metadata": {},
   "outputs": [],
   "source": [
    "main_model.predict([time_series_data[:1,:,:], current_data[:1,:]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfadd14b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1509f15",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fa813e8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "931ffaef",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cad5302",
   "metadata": {},
   "outputs": [],
   "source": [
    "actions_hist = []\n",
    "state_hist = []\n",
    "next_state_hist = []\n",
    "reward_hist = []\n",
    "gamma = 0.98\n",
    "epsilon = 0.05\n",
    "env = fake_environment()\n",
    "\n",
    "    # replay buffer \n",
    "#     actions_hist.append(current_action)\n",
    "#     state_hist.append(current_state)\n",
    "#     reward_hist.append(reward)\n",
    "#     next_state_hist.append(next_state)\n",
    "    \n",
    "# a single episode:\n",
    "\n",
    "while 1:\n",
    "\n",
    "    current_state = [time_series_data[:1,:,:], current_data[:1,:]]\n",
    "    current_action = tf.reduce_max(main_model.predict(current_state),axis=1)\n",
    "    next_state,reward,done = next(env) # next(env(current_action,current_action))\n",
    "    if done:\n",
    "        G = reward\n",
    "    else:\n",
    "        G = reward + tf.reduce_max(target_model.predict(next_state),axis=1)\n",
    "\n",
    "    with tf.GradientTape() as tape:\n",
    "        \n",
    "        tape.watch(main_model.trainable_variables)\n",
    "        current_predicted_act_values = main_model(current_state)\n",
    "        current_state_value = tf.reduce_max(current_predicted_act_values,axis=1)\n",
    "        loss = loss_function(G,current_state_value)\n",
    "        grad = tape.gradient(loss, main_model.trainable_variables)\n",
    "        optimizer.apply_gradients(zip(grad,main_model.trainable_variables))\n",
    "    \n",
    "    current_state = next_state\n",
    "    if done:\n",
    "        break\n",
    "        \n",
    "\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f14dd84",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a495ab5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cb619c0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6658b6f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "## genearte fake inputs \n",
    "from scipy.stats import norm\n",
    "sample_size, time_lenght, ts_feature_num = 1000,60,3\n",
    "current_feature_num = 5\n",
    "\n",
    "time_series_data = norm.rvs( size = [sample_size, time_lenght, ts_feature_num])\n",
    "current_data = norm.rvs(size=[sample_size, current_feature_num])\n",
    "target_data = norm.rvs(size = [sample_size,10])\n",
    "\n",
    "def fake_environment():\n",
    "    idx = 1\n",
    "    while 1:\n",
    "        if idx%5 ==0:\n",
    "            yield None,0.01, True\n",
    "        else:\n",
    "            yield  [time_series_data[idx-1:idx,:,:], current_data[idx-1:idx,:]],0.01, False\n",
    "        idx = idx+1\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dfc1a99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# perform 1 step gradient update on the model\n",
    "optimizer = keras.optimizers.RMSprop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c781124",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = fake_environment()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a664d4c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "current_state = [time_series_data[:0,:,:], current_data[:0,:]]\n",
    "# current_action = tf.reduce_max(main_model.predict(current_state),axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "966e7e69",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8de768a4",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "actions_hist = []\n",
    "state_hist = []\n",
    "next_state_hist = []\n",
    "reward_hist = []\n",
    "gamma = 0.98\n",
    "epsilon = 0.05\n",
    "env = fake_environment()\n",
    "\n",
    "    # replay buffer \n",
    "#     actions_hist.append(current_action)\n",
    "#     state_hist.append(current_state)\n",
    "#     reward_hist.append(reward)\n",
    "#     next_state_hist.append(next_state)\n",
    "    \n",
    "# a single episode:\n",
    "\n",
    "while 1:\n",
    "\n",
    "    current_state = [time_series_data[:1,:,:], current_data[:1,:]]\n",
    "    current_action = tf.reduce_max(main_model.predict(current_state),axis=1)\n",
    "    next_state,reward,done = next(env) # next(env(current_action,current_action))\n",
    "    if done:\n",
    "        G = reward\n",
    "    else:\n",
    "        G = reward + tf.reduce_max(target_model.predict(next_state),axis=1)\n",
    "\n",
    "    with tf.GradientTape() as tape:\n",
    "        \n",
    "        tape.watch(main_model.trainable_variables)\n",
    "        current_predicted_act_values = main_model(current_state)\n",
    "        current_state_value = tf.reduce_max(current_predicted_act_values,axis=1)\n",
    "        loss = loss_function(G,current_state_value)\n",
    "        grad = tape.gradient(loss, main_model.trainable_variables)\n",
    "        optimizer.apply_gradients(zip(grad,main_model.trainable_variables))\n",
    "    \n",
    "    current_state = next_state\n",
    "    if done:\n",
    "        break\n",
    "        \n",
    "\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cc2395d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:quant] *",
   "language": "python",
   "name": "conda-env-quant-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
